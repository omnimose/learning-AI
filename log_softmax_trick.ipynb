{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Log-Softmax Trick: Numerical Stability in Log-Probabilities**\n",
    "\n",
    "The **log-softmax trick** is a technique used to improve numerical stability when computing the logarithm of a softmax function. Instead of computing:\n",
    "\n",
    "$$\n",
    "\\log(\\text{softmax}(\\mathbf{z}))\n",
    "$$\n",
    "\n",
    "directly, we use a numerically stable formulation.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. The Problem: Direct Log of Softmax**\n",
    "Given a vector of logits $ \\mathbf{z} = [z_1, z_2, ..., z_n] $, the softmax function is:\n",
    "\n",
    "$$\n",
    "p_i = \\frac{\\exp(z_i)}{\\sum_j \\exp(z_j)}\n",
    "$$\n",
    "\n",
    "Taking the logarithm:\n",
    "\n",
    "$$\n",
    "\\log p_i = \\log \\left( \\frac{\\exp(z_i)}{\\sum_j \\exp(z_j)} \\right)\n",
    "$$\n",
    "\n",
    "Using the logarithm property:\n",
    "\n",
    "$$\n",
    "\\log p_i = z_i - \\log \\left( \\sum_j \\exp(z_j) \\right)\n",
    "$$\n",
    "\n",
    "However, directly computing $ \\sum_j \\exp(z_j) $ can lead to **numerical instability** when the logits are large. The exponential function grows very fast, which can cause:\n",
    "- **Overflow**: When logits are very large, $ \\exp(z_i) $ can exceed the maximum float representation.\n",
    "- **Underflow**: When logits are very small, $ \\exp(z_i) $ can become too small, leading to precision loss.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. The Trick: Log-Softmax with a Shift**\n",
    "To stabilize the computation, we subtract the **maximum logit value** from all logits before applying softmax. Define:\n",
    "\n",
    "$$\n",
    "m = \\max_j z_j\n",
    "$$\n",
    "\n",
    "Then rewrite the softmax function:\n",
    "\n",
    "$$\n",
    "p_i = \\frac{\\exp(z_i - m)}{\\sum_j \\exp(z_j - m)}\n",
    "$$\n",
    "\n",
    "Taking the logarithm:\n",
    "\n",
    "$$\n",
    "\\log p_i = (z_i - m) - \\log \\left( \\sum_j \\exp(z_j - m) \\right)\n",
    "$$\n",
    "\n",
    "This transformation **does not change the result** but improves numerical stability because:\n",
    "1. **Preventing Overflow**: Since $ z_i - m $ is at most 0 (largest logit becomes 0), exponentiation is less likely to overflow.\n",
    "2. **Preventing Underflow**: The exponentials remain in a well-behaved range.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Log-Softmax in PyTorch**\n",
    "PyTorch provides a built-in numerically stable implementation:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "logits = torch.tensor([1000.0, 1001.0, 1002.0])  # Large values cause numerical issues\n",
    "log_probs = F.log_softmax(logits, dim=-1)\n",
    "print(log_probs)\n",
    "```\n",
    "\n",
    "Instead of computing `torch.log(torch.softmax(logits, dim=-1))`, using `F.log_softmax` directly applies the trick, ensuring stability.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Summary**\n",
    "- Computing `log(softmax(x))` directly is unstable due to potential overflow/underflow in `exp(x)`.\n",
    "- The **log-softmax trick** rewrites the equation by subtracting the max logit value before exponentiation.\n",
    "- PyTorchâ€™s `F.log_softmax()` is numerically stable and should be preferred over `torch.log(torch.softmax())`.\n",
    "\n",
    "This trick is crucial in deep learning, particularly in **categorical cross-entropy loss** and **policy gradient methods** like PPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.4076, -1.4076, -0.4076])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "logits = torch.tensor([1000.0, 1001.0, 1002.0])  # Large values cause numerical issues\n",
    "log_probs = F.log_softmax(logits, dim=-1)\n",
    "print(log_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-Softmax values: tensor([-0.4170, -1.4170, -2.3170], grad_fn=<LogSoftmaxBackward0>)\n",
      "Gradients: tensor([ 0.3410, -0.2424, -0.0986])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "logits = torch.tensor([2.0, 1.0, 0.1], requires_grad=True)  # Example logits\n",
    "log_probs = F.log_softmax(logits, dim=-1)  # Compute log-softmax directly\n",
    "chosen_action = 0  # Let's say action 0 was chosen\n",
    "\n",
    "log_prob = log_probs[chosen_action]  # Extract log-probability of chosen action\n",
    "log_prob.backward()  # Compute gradients\n",
    "\n",
    "print(\"Log-Softmax values:\", log_probs)\n",
    "print(\"Gradients:\", logits.grad)  # Should match (delta_i,a - p_i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax Probabilities: tensor([0.6590, 0.2424, 0.0986], grad_fn=<DivBackward0>)\n",
      "Log-Softmax Values: tensor([-0.4170, -1.4170, -2.3170], grad_fn=<SubBackward0>)\n",
      "Manual Gradients: tensor([ 0.3410, -0.2424, -0.0986], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define logits\n",
    "torch.manual_seed(42)\n",
    "logits = torch.tensor([2.0, 1.0, 0.1], requires_grad=True)  # Example logits\n",
    "\n",
    "# Step 1: Compute Softmax Probabilities\n",
    "exp_logits = torch.exp(logits)\n",
    "sum_exp_logits = torch.sum(exp_logits)\n",
    "probs = exp_logits / sum_exp_logits  # Softmax probabilities\n",
    "\n",
    "# Step 2: Compute Log-Softmax\n",
    "log_probs = logits - torch.log(sum_exp_logits)  # Log-softmax values\n",
    "\n",
    "# Choose an action (action 0)\n",
    "chosen_action = 0\n",
    "log_prob = log_probs[chosen_action]\n",
    "\n",
    "# Step 3: Compute Gradients Manually\n",
    "grad_manual = torch.eye(len(probs))[chosen_action] - probs  # (delta_i,a - p_i)\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(\"Softmax Probabilities:\", probs)\n",
    "print(\"Log-Softmax Values:\", log_probs)\n",
    "print(\"Manual Gradients:\", grad_manual)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
