{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, `torch.nn.NLLLoss` (Negative Log-Likelihood Loss) is typically used in classification problems, particularly when working with the output of a `log_softmax` activation function. It computes the negative log likelihood of the target classes, given the predicted log-probabilities.\n",
    "\n",
    "\n",
    "### Explanation:\n",
    "1. **Input (logits)**: The model's raw output (before applying any activation like softmax).\n",
    "2. **Log softmax**: Converts logits into log-probabilities.\n",
    "3. **Targets**: The ground truth class labels (0-indexed).\n",
    "4. **Loss**: The `NLLLoss` function computes the negative log-likelihood for each sample and returns the mean loss.\n",
    "\n",
    "Make sure the model output uses `log_softmax` before passing it to `NLLLoss`. If your model outputs raw logits, `log_softmax` is necessary to get proper probabilities.\n",
    "\n",
    "The formula for the **Negative Log-Likelihood Loss** (NLLLoss) is given by:\n",
    "\n",
    "$$\n",
    "\\text{NLLLoss}(x, y) = - \\log(p(y | x))\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( x \\) is the input to the model (e.g., logits).\n",
    "- \\( y \\) is the ground truth (the target class).\n",
    "- \\( p(y | x) \\) is the predicted probability of the correct class \\( y \\), based on the input \\( x \\).\n",
    "\n",
    "For multi-class classification, if the model output is the **logits** (i.e., unnormalized scores), we first apply the **softmax** function to obtain the predicted probabilities, and then compute the negative log of the probability of the correct class.\n",
    "\n",
    "Mathematically, for the **logits** \\( z_i \\) (where \\( i \\) is the class index), the softmax function is defined as:\n",
    "\n",
    "$$\n",
    "p(y = j | x) = \\frac{e^{z_j}}{\\sum_{k=1}^{C} e^{z_k}}\n",
    "$$\n",
    "\n",
    "Where \\( C \\) is the number of classes, and \\( z_j \\) is the logit corresponding to class \\( j \\).\n",
    "\n",
    "The **NLLLoss** for a single sample is then:\n",
    "\n",
    "$$\n",
    "\\text{NLLLoss}(z, y) = -\\log(p(y = y_{\\text{true}} | x)) = -\\log\\left(\\frac{e^{z_{y_{\\text{true}}}}}{\\sum_{k=1}^{C} e^{z_k}}\\right)\n",
    "$$\n",
    "\n",
    "This loss is averaged over all samples in the batch during training.\n",
    "\n",
    "### Key Points:\n",
    "1. The model outputs logits (raw scores before activation).\n",
    "2. The softmax function converts logits into probabilities.\n",
    "3. The log of the predicted probability for the correct class is taken.\n",
    "4. The negative of this log is the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8701, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the NLLLoss function\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Example of logits (output from log_softmax)\n",
    "logits = torch.tensor([[0.2, 0.5, -0.1], [-0.1, 0.1, 0.3]], requires_grad=True)  # 2 samples, 3 classes\n",
    "log_probs = torch.log_softmax(logits, dim=1)  # Convert logits to log probabilities\n",
    "\n",
    "# Example of target labels\n",
    "targets = torch.tensor([1, 2])  # Correct classes for each sample (class 1 for the first sample, class 2 for the second)\n",
    "\n",
    "# Compute the loss\n",
    "loss = criterion(log_probs, targets)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To manually calculate the **Negative Log-Likelihood Loss (NLLLoss)** for the given data, let's go through each step using the provided values.\n",
    "\n",
    "### **Given values:**\n",
    "- **Logits**: \n",
    "  $$\n",
    "  \\text{logits} = \\begin{bmatrix}\n",
    "  0.2 & 0.5 & -0.1 \\\\\n",
    "  -0.1 & 0.1 & 0.3\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "  \n",
    "- **Targets**: \n",
    "  $$\n",
    "  \\text{targets} = [1, 2]\n",
    "  $$\n",
    "\n",
    "### **Step 1: Apply Softmax to the logits**\n",
    "To compute log-probabilities, we first apply the **softmax** function to the logits. The softmax function transforms logits into probabilities.\n",
    "\n",
    "The softmax function for a vector \\(z = [z_1, z_2, z_3]\\) is given by:\n",
    "\n",
    "$$\n",
    "p(y = j | x) = \\frac{e^{z_j}}{\\sum_{k=1}^{C} e^{z_k}}\n",
    "$$\n",
    "\n",
    "For each row of logits, we apply softmax:\n",
    "\n",
    "#### **First row: [0.2, 0.5, -0.1]**\n",
    "$$\n",
    "\\text{softmax}(0.2, 0.5, -0.1) = \\left[ \\frac{e^{0.2}}{e^{0.2} + e^{0.5} + e^{-0.1}}, \\frac{e^{0.5}}{e^{0.2} + e^{0.5} + e^{-0.1}}, \\frac{e^{-0.1}}{e^{0.2} + e^{0.5} + e^{-0.1}} \\right]\n",
    "$$\n",
    "Calculate exponentials and normalize to obtain probabilities.\n",
    "\n",
    "#### **Second row: [-0.1, 0.1, 0.3]**\n",
    "$$\n",
    "\\text{softmax}(-0.1, 0.1, 0.3) = \\left[ \\frac{e^{-0.1}}{e^{-0.1} + e^{0.1} + e^{0.3}}, \\frac{e^{0.1}}{e^{-0.1} + e^{0.1} + e^{0.3}}, \\frac{e^{0.3}}{e^{-0.1} + e^{0.1} + e^{0.3}} \\right]\n",
    "$$\n",
    "Similarly, calculate exponentials and normalize to get the probabilities.\n",
    "\n",
    "### **Step 2: Compute Log-Probabilities**\n",
    "Once we have the softmax probabilities, we apply the natural logarithm to these probabilities to obtain log-probabilities.\n",
    "\n",
    "### **Step 3: Calculate NLL Loss**\n",
    "Now, the **NLL Loss** for each sample is computed as:\n",
    "\n",
    "$$\n",
    "\\text{NLLLoss}(x, y) = - \\log(p(y_{\\text{true}} | x))\n",
    "$$\n",
    "\n",
    "For the given targets:\n",
    "- For the first sample, the target class is 1 (index 1 in 0-indexed).\n",
    "- For the second sample, the target class is 2 (index 2 in 0-indexed).\n",
    "\n",
    "Thus, we calculate the negative log of the predicted probability for the correct class for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits:\n",
      " tensor([[ 0.2000,  0.5000, -0.1000],\n",
      "        [-0.1000,  0.1000,  0.3000]])\n",
      "Probabilities:\n",
      " tensor([[0.3236, 0.4368, 0.2397],\n",
      "        [0.2693, 0.3289, 0.4018]])\n",
      "Log-Probabilities:\n",
      " tensor([[-1.1284, -0.8284, -1.4284],\n",
      "        [-1.3119, -1.1119, -0.9119]])\n",
      "Negative Log-Likelihoods for each sample: tensor([0.8284, 0.9119])\n",
      "Mean NLL Loss: 0.8701457977294922\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Given logits\n",
    "logits = torch.tensor([[0.2, 0.5, -0.1], [-0.1, 0.1, 0.3]])\n",
    "\n",
    "# Targets (true classes)\n",
    "targets = torch.tensor([1, 2])\n",
    "\n",
    "# Step 1: Apply Softmax to logits to get probabilities\n",
    "probabilities = F.softmax(logits, dim=1)\n",
    "\n",
    "# Step 2: Get the log-probabilities by taking the log of probabilities\n",
    "log_probs = torch.log(probabilities)\n",
    "\n",
    "# Step 3: Compute the negative log-likelihood for the correct class (using target labels)\n",
    "# For each sample, we pick the log-probability of the correct class and take the negative\n",
    "nll_losses = -log_probs[torch.arange(len(targets)), targets]\n",
    "\n",
    "# Final NLL Loss is the mean of the losses\n",
    "nll_loss = nll_losses.mean()\n",
    "\n",
    "print(\"Logits:\\n\", logits)\n",
    "print(\"Probabilities:\\n\", probabilities)\n",
    "print(\"Log-Probabilities:\\n\", log_probs)\n",
    "print(\"Negative Log-Likelihoods for each sample:\", nll_losses)\n",
    "print(\"Mean NLL Loss:\", nll_loss.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
