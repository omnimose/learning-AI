{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " NLL Loss and Cross Entropy Loss are closely related in PyTorch and in machine learning generally. In fact, CrossEntropyLoss is a combination of LogSoftmax and NLLLoss in a single function.\n",
    " \n",
    "Key Points about nll_loss:\n",
    "- NLL Loss (Negative Log Likelihood Loss) is often used in classification problems where the model outputs log probabilities (often the output from log_softmax).\n",
    "- The function compares the log-probabilities from the model to the actual target labels.\n",
    "- This loss function expects the input to be log probabilities and the target to be class indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CrossEntropyLoss(logits,target)=NLLLoss(log(softmax(logits)),target)\n",
    "\n",
    "training:\n",
    "- logits -> cross_entropy \n",
    "- logits -> log_softmax -> nll_loss\n",
    "- logits -> softmax -> log -> nll_loss (Not optimal and should be avoided due to inefficiency and numerical instability.)\n",
    "\n",
    "inference:\n",
    "- logits -> softmax -> multinomial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_probs:\n",
      " tensor([[-0.6041, -1.1041, -2.1041],\n",
      "        [-0.1948, -3.1948, -1.9948],\n",
      "        [-2.5836, -0.1836, -2.3836],\n",
      "        [-0.8859, -1.3859, -1.0859]])\n",
      "NLL Loss: 0.9171183109283447\n",
      "Manual NLL Loss: 0.9171\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Step 1: Define some arbitrary logits\n",
    "logits = torch.tensor([\n",
    "    [2.0, 1.5, 0.5],  # Sample 1\n",
    "    [3.0, 0.0, 1.2],  # Sample 2\n",
    "    [0.1, 2.5, 0.3],  # Sample 3\n",
    "    [1.5, 1.0, 1.3]   # Sample 4\n",
    "])\n",
    "\n",
    "# Step 2: Apply softmax\n",
    "softmax_probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "\n",
    "# Step 3: Apply log\n",
    "log_probs = torch.log(softmax_probs)\n",
    "\n",
    "# Print the correct log_probs tensor\n",
    "print(\"log_probs:\\n\", log_probs)\n",
    "\n",
    "# Target labels for each sample (true class indices)\n",
    "targets = torch.tensor([0, 2, 1, 0])\n",
    "\n",
    "# Calculate NLL Loss\n",
    "loss = F.nll_loss(log_probs, targets)\n",
    "\n",
    "print(f\"NLL Loss: {loss.item()}\")\n",
    "\n",
    "# manual calculation\n",
    "manual_loss = -((-0.6041) + (-1.9948) + (-0.1836) + (-0.8859)) / 4\n",
    "print(f\"Manual NLL Loss: {manual_loss}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3743)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Instantiate the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Sample logits and labels\n",
    "logits = torch.tensor([[1.0, 2.0, 0.1],\n",
    "                       [1.2, 0.5, 0.3],\n",
    "                       [0.4, 1.0, 1.5]], dtype=torch.float32)\n",
    "labels = torch.tensor([2, 0, 1], dtype=torch.long)\n",
    "\n",
    "# Call the instantiated object to compute the loss\n",
    "loss = criterion(logits, labels)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3617)\n",
      "tensor(0.3617)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "logits = torch.tensor([[2.0, 1.0, 0.1], \n",
    "                       [0.5, 2.5, 1.0]])  # Raw scores\n",
    "\n",
    "targets = torch.tensor([0, 1])  # Class labels\n",
    "\n",
    "# Using CrossEntropyLoss\n",
    "loss_fn_ce = nn.CrossEntropyLoss()\n",
    "loss_ce = loss_fn_ce(logits, targets)\n",
    "\n",
    "# Equivalent using LogSoftmax + NLLLoss\n",
    "log_softmax = torch.log_softmax(logits, dim=1)  # Log probabilities\n",
    "loss_fn_nll = nn.NLLLoss()\n",
    "loss_nll = loss_fn_nll(log_softmax, targets)\n",
    "\n",
    "print(loss_ce)   # Same value\n",
    "print(loss_nll)  # Same value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2424, 0.6590, 0.0986],\n",
      "        [0.5254, 0.2609, 0.2136],\n",
      "        [0.1716, 0.3127, 0.5156]])\n",
      "tensor([[-1.4170, -0.4170, -2.3170],\n",
      "        [-0.6435, -1.3435, -1.5435],\n",
      "        [-1.7624, -1.1624, -0.6624]])\n",
      "incorrect_nll:  tensor(-0.3123)\n",
      "correct_nll: tensor(1.3743)\n",
      "cross entropy:  tensor(1.3743)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "logits = torch.tensor([[1.0, 2.0, 0.1],\n",
    "                       [1.2, 0.5, 0.3],\n",
    "                       [0.4, 1.0, 1.5]], dtype=torch.float32)\n",
    "\n",
    "sm = F.softmax(logits, dim=-1)\n",
    "print(sm)\n",
    "\n",
    "lsm = F.log_softmax(logits, dim=-1)\n",
    "print(lsm)\n",
    "\n",
    "labels = torch.tensor([2, 0, 1], dtype=torch.long)\n",
    "\n",
    "# incorrect\n",
    "incorrect_nll = F.nll_loss(sm, labels)\n",
    "\n",
    "# correct, we should use log_softmax\n",
    "correct_nll = F.nll_loss(lsm, labels)\n",
    "\n",
    "print(\"incorrect_nll: \", incorrect_nll)\n",
    "print(\"correct_nll:\", correct_nll)\n",
    "\n",
    "cross_entropy = F.cross_entropy(logits, labels)\n",
    "print(\"cross entropy: \", cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1:\n",
      "tensor([[0.2424, 0.6590, 0.0986],\n",
      "        [0.5254, 0.2609, 0.2136],\n",
      "        [0.1716, 0.3127, 0.5156]])\n",
      "t1.log():\n",
      "tensor([[-1.4170, -0.4170, -2.3170],\n",
      "        [-0.6435, -1.3435, -1.5435],\n",
      "        [-1.7624, -1.1624, -0.6624]])\n",
      "torch.log(t1):\n",
      "tensor([[-1.4170, -0.4170, -2.3170],\n",
      "        [-0.6435, -1.3435, -1.5435],\n",
      "        [-1.7624, -1.1624, -0.6624]])\n",
      "log_softmax:\n",
      "tensor([[-1.4170, -0.4170, -2.3170],\n",
      "        [-0.6435, -1.3435, -1.5435],\n",
      "        [-1.7624, -1.1624, -0.6624]])\n"
     ]
    }
   ],
   "source": [
    "logits = torch.tensor([[1.0, 2.0, 0.1],\n",
    "                       [1.2, 0.5, 0.3],\n",
    "                       [0.4, 1.0, 1.5]], dtype=torch.float32)\n",
    "\n",
    "t1 = F.softmax(logits, dim=-1)\n",
    "print(\"t1:\")\n",
    "print(t1)\n",
    "\n",
    "t2 = t1.log()\n",
    "print(\"t1.log():\")\n",
    "print(t2)\n",
    "\n",
    "t3 = torch.log(t1)\n",
    "print(\"torch.log(t1):\")\n",
    "print(t3)\n",
    "\n",
    "print(\"log_softmax:\")\n",
    "t4 = F.log_softmax(logits, dim=-1)\n",
    "print(t4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
