{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term **logits** comes from **log-odds**, which is related to how logits are used in machine learning, particularly in classification tasks.\n",
    "\n",
    "### Breaking Down the Name \"Logits\":\n",
    "\n",
    "1. **Log-Odds**:\n",
    "   - The **odds** of an event happening are the ratio of the probability of the event happening to the probability of it not happening.\n",
    "     $$\n",
    "     \\text{Odds}(p) = \\frac{p}{1 - p}\n",
    "     $$\n",
    "   - The **log-odds** (or **logit**) is simply the logarithm of the odds:\n",
    "     $$\n",
    "     \\text{log-odds}(p) = \\log\\left(\\frac{p}{1 - p}\\right)\n",
    "     $$\n",
    "   - This transforms a probability (which is between 0 and 1) into a value that can range from $-\\infty$ to $+\\infty$.\n",
    "\n",
    "2. **Logits in Machine Learning**:\n",
    "   - In the context of machine learning, **logits** refer to the unnormalized scores that are often passed to a softmax function for multi-class classification. These logits represent the model's confidence in each class before being converted into probabilities.\n",
    "   - The idea is that logits can be interpreted similarly to **log-odds**, though they arenâ€™t exactly log-odds in most cases. They are raw scores that, when passed through softmax, give the equivalent of probabilities for each class.\n",
    "   - In binary classification using logistic regression, the raw output is indeed the log-odds (hence the term **logit**), which is then converted to a probability using the **sigmoid** function. For multi-class classification, the softmax function generalizes this concept.\n",
    "\n",
    "3. **Logistic Function**:\n",
    "   - The **logistic function** (used in logistic regression) converts log-odds into probabilities:\n",
    "     $$\n",
    "     \\text{sigmoid}(z) = \\frac{1}{1 + e^{-z}}\n",
    "     $$\n",
    "     This function maps a logit (which can be any real number) into a probability between 0 and 1.\n",
    "   - In **binary classification**, the output of a model is often interpreted as log-odds, and the logistic function converts those into probabilities, making the term **logit** a natural name.\n",
    "\n",
    "### Why \"Logits\" in Modern Deep Learning:\n",
    "- While in deep learning (especially with softmax), logits are not directly the log-odds, the term is still used to refer to the **raw, unnormalized scores** that precede the probability computation.\n",
    "- The name **logits** stuck because it captures the idea that these are intermediate values that can be transformed into probabilities through a suitable function (like softmax or sigmoid).\n",
    "\n",
    "### Summary:\n",
    "The name **logits** comes from the concept of **log-odds** in statistics. Even though modern logits are not always log-odds, the term is used to describe the unnormalized scores output by models before they are transformed into probabilities using functions like softmax or sigmoid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To explain how torch.nn.functional.nll_loss works, let's walk through a simple example with some sample data.\n",
    "\n",
    "Key Points about nll_loss:\n",
    "- NLL Loss (Negative Log Likelihood Loss) is often used in classification problems where the model outputs log probabilities (often the output from log_softmax).\n",
    "- The function compares the log-probabilities from the model to the actual target labels.\n",
    "- This loss function expects the input to be log probabilities and the target to be class indices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
