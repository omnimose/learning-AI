{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mainly generated by chatgpt: https://chatgpt.com/share/6711d36b-2da8-8004-ad10-37341331c93a\n",
    "\n",
    "A Conv2d layer applies convolutional filters to an input, which can be thought of as sliding a filter (kernel) over the input image to produce an output called the feature map.\n",
    "\n",
    "Key Parameters of torch.nn.Conv2d\n",
    "- in_channels: Number of channels in the input image (e.g., 3 for RGB images).\n",
    "- out_channels: Number of filters (feature detectors) the convolution will apply.\n",
    "- kernel_size: Size of the filter (e.g., 3x3 or 5x5).\n",
    "- stride: Step size for the filter (default: 1).\n",
    "- padding: Adds padding around the input image (default: 0).\n",
    "\n",
    "Example:\n",
    "1. Input tensor shape:\n",
    "\n",
    "- Let's assume an input of size (1, 1, 5, 5). This is a batch of 1 image, with 1 channel (grayscale), and a 5x5 pixel grid.\n",
    "\n",
    "2. Filter (kernel):\n",
    "\n",
    "- We will use a 3x3 filter (kernel) and set out_channels=1, which means the convolution will produce a single output feature map.\n",
    "Hereâ€™s a code snippet to demonstrate how it works:\n",
    "\n",
    "reference:\n",
    "https://www.youtube.com/watch?v=n8Mey4o8gLc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor shape: torch.Size([1, 1, 5, 5])\n",
      "Output tensor shape: torch.Size([1, 1, 3, 3])\n",
      "Output feature map:\n",
      " tensor([[[[-2., -2.,  4.],\n",
      "          [-3., -2.,  3.],\n",
      "          [ 0., -2.,  1.]]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Input tensor of shape (batch_size, in_channels, height, width)\n",
    "input_tensor = torch.tensor([[[[1.0, 2.0, 3.0, 0.0, 1.0],\n",
    "                               [0.0, 1.0, 2.0, 3.0, 0.0],\n",
    "                               [3.0, 0.0, 1.0, 2.0, 1.0],\n",
    "                               [0.0, 2.0, 3.0, 0.0, 2.0],\n",
    "                               [1.0, 1.0, 0.0, 3.0, 0.0]]]])\n",
    "\n",
    "# Conv2d layer with 1 input channel, 1 output channel, and 3x3 kernel\n",
    "conv_layer = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=0)\n",
    "\n",
    "# Initialize weights and bias (for demonstration)\n",
    "conv_layer.weight = torch.nn.Parameter(torch.tensor([[[[1.0, 0.0, -1.0],\n",
    "                                                       [1.0, 0.0, -1.0],\n",
    "                                                       [1.0, 0.0, -1.0]]]]))\n",
    "conv_layer.bias = torch.nn.Parameter(torch.tensor([0.0]))\n",
    "\n",
    "# Apply the convolution\n",
    "output = conv_layer(input_tensor)\n",
    "\n",
    "print(\"Input tensor shape:\", input_tensor.shape)\n",
    "print(\"Output tensor shape:\", output.shape)\n",
    "print(\"Output feature map:\\n\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually verify the output of a Conv2d layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element-wise product:\n",
      " tensor([[ 1.,  0., -3.],\n",
      "        [ 0.,  0., -2.],\n",
      "        [ 3.,  0., -1.]])\n",
      "Sum of element-wise product: tensor(-2.)\n",
      "Element-wise product:\n",
      " tensor([[ 2.,  0., -0.],\n",
      "        [ 1.,  0., -3.],\n",
      "        [ 0.,  0., -2.]])\n",
      "Sum of element-wise product: tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "# verification\n",
    "tensor1 = torch.tensor([[1.0, 2.0, 3.0],\n",
    "                        [0.0, 1.0, 2.0],\n",
    "                        [3.0, 0.0, 1.0]])\n",
    "\n",
    "tensor2 = torch.tensor([[2.0, 3.0, 0.0],\n",
    "                        [1.0, 2.0, 3.0],\n",
    "                        [0.0, 1.0, 2.0]])\n",
    "\n",
    "kernel = torch.tensor([[1.0, 0.0, -1.0],\n",
    "                      [1.0, 0.0, -1.0],\n",
    "                      [1.0, 0.0, -1.0]])\n",
    "\n",
    "# Element-wise multiplication\n",
    "elementwise_product1 = torch.mul(tensor1, kernel)\n",
    "# Sum of the element-wise multiplication\n",
    "sum_result1 = torch.sum(elementwise_product1)\n",
    "print(\"Element-wise product:\\n\", elementwise_product1)\n",
    "print(\"Sum of element-wise product:\", sum_result1)\n",
    "\n",
    "# Element-wise multiplication\n",
    "elementwise_product2 = torch.mul(tensor2, kernel)\n",
    "# Sum of the element-wise multiplication\n",
    "sum_result2 = torch.sum(elementwise_product2)\n",
    "print(\"Element-wise product:\\n\", elementwise_product2)\n",
    "print(\"Sum of element-wise product:\", sum_result2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "# sample with padding\n",
    "# Define the input tensor (batch_size, channels, height, width)\n",
    "input_tensor = torch.randn(1, 1, 5, 5)  # Example: batch_size = 1, 1 channel, 5x5 image\n",
    "\n",
    "# Define the convolutional layer\n",
    "conv_layer = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "# Perform the convolution operation\n",
    "output_tensor = conv_layer(input_tensor)\n",
    "\n",
    "print(output_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor shape: torch.Size([1, 3, 3, 3])\n",
      "Output tensor shape: torch.Size([1, 2, 1, 1])\n",
      "Output feature maps:\n",
      " tensor([[[[11.]],\n",
      "\n",
      "         [[ 1.]]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# sample with 3 input channels, 2 output channels\n",
    "# Input tensor of shape (batch_size, in_channels, height, width)\n",
    "# Let's assume batch_size=1, in_channels=3 (RGB image), and each channel is 3x3\n",
    "input_tensor = torch.tensor([[[[1.0, 2.0, 3.0],\n",
    "                               [4.0, 5.0, 6.0],\n",
    "                               [7.0, 8.0, 9.0]],   # Red Channel\n",
    "                             \n",
    "                              [[9.0, 8.0, 7.0],\n",
    "                               [6.0, 5.0, 4.0],\n",
    "                               [3.0, 2.0, 1.0]],   # Green Channel\n",
    "                             \n",
    "                              [[0.0, 1.0, 0.0],\n",
    "                               [1.0, 0.0, 1.0],\n",
    "                               [0.0, 1.0, 0.0]]]]) # Blue Channel\n",
    "\n",
    "# Conv2d layer with 3 input channels and 2 output channels, kernel size 3x3\n",
    "conv_layer = nn.Conv2d(in_channels=3, out_channels=2, kernel_size=3, stride=1, padding=0)\n",
    "\n",
    "# Initialize weights and bias for the 2 output channels (filters)\n",
    "conv_layer.weight = torch.nn.Parameter(torch.tensor([[[[1.0, 0.0, -1.0],\n",
    "                                                       [1.0, 0.0, -1.0],\n",
    "                                                       [1.0, 0.0, -1.0]],   # Filter for Red Channel\n",
    "\n",
    "                                                      [[0.0, 1.0, 0.0],\n",
    "                                                       [0.0, 1.0, 0.0],\n",
    "                                                       [0.0, 1.0, 0.0]],   # Filter for Green Channel\n",
    "                                                    \n",
    "                                                      [[1.0, 0.0, 1.0],\n",
    "                                                       [1.0, 0.0, 1.0],\n",
    "                                                       [1.0, 0.0, 1.0]]],  # Filter for Blue Channel\n",
    "\n",
    "                                                     [[[1.0, -1.0, 1.0],\n",
    "                                                       [1.0, -1.0, 1.0],\n",
    "                                                       [1.0, -1.0, 1.0]],  # Filter for Red Channel\n",
    "\n",
    "                                                      [[-1.0, 1.0, -1.0],\n",
    "                                                       [-1.0, 1.0, -1.0],\n",
    "                                                       [-1.0, 1.0, -1.0]], # Filter for Green Channel\n",
    "\n",
    "                                                      [[0.0, 0.0, 1.0],\n",
    "                                                       [0.0, 0.0, 1.0],\n",
    "                                                       [0.0, 0.0, 1.0]]]]))  # Filter for Blue Channel\n",
    "\n",
    "conv_layer.bias = torch.nn.Parameter(torch.tensor([0.0, 0.0])) # Bias for both filters\n",
    "\n",
    "# Apply the convolution\n",
    "output = conv_layer(input_tensor)\n",
    "\n",
    "print(\"Input tensor shape:\", input_tensor.shape)\n",
    "print(\"Output tensor shape:\", output.shape)\n",
    "print(\"Output feature maps:\\n\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element-wise product:\n",
      " tensor([[ 1.,  0., -3.],\n",
      "        [ 4.,  0., -6.],\n",
      "        [ 7.,  0., -9.]])\n",
      "Sum of element-wise product: tensor(-6.)\n",
      "Element-wise product:\n",
      " tensor([[0., 8., 0.],\n",
      "        [0., 5., 0.],\n",
      "        [0., 2., 0.]])\n",
      "Sum of element-wise product: tensor(15.)\n",
      "Element-wise product:\n",
      " tensor([[0., 0., 0.],\n",
      "        [1., 0., 1.],\n",
      "        [0., 0., 0.]])\n",
      "Sum of element-wise product: tensor(2.)\n",
      "Sum of all channels: tensor(11.)\n"
     ]
    }
   ],
   "source": [
    "# verification\n",
    "r = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6],\n",
    "                  [7, 8, 9]])\n",
    "\n",
    "r_filter1 = torch.tensor([[1.0, 0.0, -1.0],\n",
    "                          [1.0, 0.0, -1.0],\n",
    "                          [1.0, 0.0, -1.0]])\n",
    "\n",
    "r_product1 = torch.mul(r, r_filter1)\n",
    "r_sum1 = torch.sum(r_product1)\n",
    "print(\"Element-wise product:\\n\", r_product1)\n",
    "print(\"Sum of element-wise product:\", r_sum1)\n",
    "\n",
    "g = torch.tensor([[9.0, 8.0, 7.0],\n",
    "                  [6.0, 5.0, 4.0],\n",
    "                  [3.0, 2.0, 1.0]])\n",
    "\n",
    "g_filter1 = torch.tensor([[0.0, 1.0, 0.0],\n",
    "                          [0.0, 1.0, 0.0],\n",
    "                          [0.0, 1.0, 0.0]])\n",
    "\n",
    "g_product1 = torch.mul(g, g_filter1)\n",
    "g_sum1 = torch.sum(g_product1)\n",
    "print(\"Element-wise product:\\n\", g_product1)\n",
    "print(\"Sum of element-wise product:\", g_sum1)\n",
    "\n",
    "b = torch.tensor([[0.0, 1.0, 0.0],\n",
    "                  [1.0, 0.0, 1.0],\n",
    "                  [0.0, 1.0, 0.0]])\n",
    "\n",
    "b_filter1 = torch.tensor([[1.0, 0.0, 1.0],\n",
    "                          [1.0, 0.0, 1.0],\n",
    "                          [1.0, 0.0, 1.0]])\n",
    "\n",
    "b_product1 = torch.mul(b, b_filter1)\n",
    "b_sum1 = torch.sum(b_product1)\n",
    "print(\"Element-wise product:\\n\", b_product1)\n",
    "print(\"Sum of element-wise product:\", b_sum1)\n",
    "\n",
    "sum = r_sum1 + g_sum1 + b_sum1\n",
    "print(\"Sum of all channels:\", sum)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor shape: torch.Size([1, 3, 3, 3])\n",
      "Output tensor shape: torch.Size([1, 3, 1, 1])\n",
      "Output feature maps:\n",
      " tensor([[[[11.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[26.]]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Input tensor of shape (batch_size, in_channels, height, width)\n",
    "# Let's assume batch_size=1, in_channels=3 (RGB image), and each channel is 3x3\n",
    "input_tensor = torch.tensor([[[[1.0, 2.0, 3.0],\n",
    "                               [4.0, 5.0, 6.0],\n",
    "                               [7.0, 8.0, 9.0]],  # Red Channel\n",
    "                             \n",
    "                              [[9.0, 8.0, 7.0],\n",
    "                               [6.0, 5.0, 4.0],\n",
    "                               [3.0, 2.0, 1.0]],  # Green Channel\n",
    "                             \n",
    "                              [[0.0, 1.0, 0.0],\n",
    "                               [1.0, 0.0, 1.0],\n",
    "                               [0.0, 1.0, 0.0]]]])  # Blue Channel\n",
    "\n",
    "# Conv2d layer with 3 input channels and 3 output channels, kernel size 3x3\n",
    "conv_layer = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=0)\n",
    "\n",
    "# Initialize weights for 3 output channels (each has 3 filters for 3 input channels)\n",
    "conv_layer.weight = torch.nn.Parameter(torch.tensor([\n",
    "    # Filters for output channel 1\n",
    "    [[[1.0, 0.0, -1.0], [1.0, 0.0, -1.0], [1.0, 0.0, -1.0]],   # Filter for Red Channel\n",
    "     [[0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0]],     # Filter for Green Channel\n",
    "     [[1.0, 0.0, 1.0], [1.0, 0.0, 1.0], [1.0, 0.0, 1.0]]],    # Filter for Blue Channel\n",
    "    \n",
    "    # Filters for output channel 2\n",
    "    [[[1.0, -1.0, 1.0], [1.0, -1.0, 1.0], [1.0, -1.0, 1.0]],  # Filter for Red Channel\n",
    "     [[-1.0, 1.0, -1.0], [-1.0, 1.0, -1.0], [-1.0, 1.0, -1.0]], # Filter for Green Channel\n",
    "     [[0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0]]],   # Filter for Blue Channel\n",
    "    \n",
    "    # Filters for output channel 3\n",
    "    [[[0.0, 1.0, 0.0], [1.0, 0.0, 1.0], [0.0, 1.0, 0.0]],     # Filter for Red Channel\n",
    "     [[1.0, 0.0, -1.0], [1.0, 0.0, -1.0], [1.0, 0.0, -1.0]],   # Filter for Green Channel\n",
    "     [[-1.0, 1.0, -1.0], [-1.0, 1.0, -1.0], [-1.0, 1.0, -1.0]]]]))  # Filter for Blue Channel\n",
    "\n",
    "conv_layer.bias = torch.nn.Parameter(torch.tensor([0.0, 0.0, 0.0]))  # Bias for all 3 output channels\n",
    "\n",
    "# Apply the convolution\n",
    "output = conv_layer(input_tensor)\n",
    "\n",
    "print(\"Input tensor shape:\", input_tensor.shape)\n",
    "print(\"Output tensor shape:\", output.shape)\n",
    "print(\"Output feature maps:\\n\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.nn.functional.conv2d\n",
    "Key points:\n",
    "- Input tensor: Needs to be of shape (batch_size, in_channels, height, width).\n",
    "- Kernel: Needs to be of shape (out_channels, in_channels, kernel_height, kernel_width).\n",
    "- Output: The result of the convolution is an output tensor of reduced size, depending on the kernel size and stride."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# nn.functional sample\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the input tensor (batch_size, channels, height, width)\n",
    "input_tensor = torch.randn(1, 1, 5, 5)  # Example: batch_size=1, 1 channel, 5x5 image\n",
    "\n",
    "# Define the kernel (filter)\n",
    "kernel = torch.randn(1, 1, 3, 3)  # 1 output channel, 1 input channel, 3x3 kernel\n",
    "\n",
    "# Perform the convolution (with no padding or stride by default)\n",
    "output_tensor = F.conv2d(input_tensor, kernel, stride=1, padding=0)\n",
    "\n",
    "print(output_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NumPy, you can perform convolution using the numpy.convolve() function for 1D arrays, or numpy's scipy.signal.convolve() for higher-dimensional arrays (e.g., 2D or 3D)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  1.  2.5 4.  1.5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define two 1D arrays\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([0, 1, 0.5])\n",
    "\n",
    "# Perform convolution\n",
    "result = np.convolve(a, b, mode='full')\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters of numpy.convolve():\n",
    "- a: First input array.\n",
    "- b: Second input array (the kernel).\n",
    "- mode:\n",
    "  - 'full' (default): Returns the full convolution result.\n",
    "  - 'valid': Returns only elements where the arrays fully overlap.\n",
    "  - 'same': Returns the output of the same size as a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2   1  -4]\n",
      " [ -3   0  -7]\n",
      " [-16 -11 -22]]\n"
     ]
    }
   ],
   "source": [
    "from scipy import signal\n",
    "import numpy as np\n",
    "\n",
    "# Define a 2D array (input image)\n",
    "a = np.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9]])\n",
    "\n",
    "# Define a 2D kernel\n",
    "kernel = np.array([[0, 1, 0],\n",
    "                   [1, -4, 1],\n",
    "                   [0, 1, 0]])\n",
    "\n",
    "# Perform 2D convolution\n",
    "result = signal.convolve(a, kernel, mode='same')\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step-by-Step Process:\n",
    "1. Padding the Input Array:\n",
    "\n",
    "Since we're using mode='same', the output array will have the same size as the input array (3x3). To achieve this, we need to pad the input array so that the kernel can be applied to every element in the original array without going out of bounds.\n",
    "The kernel is 3x3, so we pad the input array with a 1-pixel border of zeros. The padding is necessary because the kernel will partially overlap the edges of the input.\n",
    "\n",
    "The padded input array looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[0, 0, 0, 0, 0],\n",
    " [0, 1, 2, 3, 0],\n",
    " [0, 4, 5, 6, 0],\n",
    " [0, 7, 8, 9, 0],\n",
    " [0, 0, 0, 0, 0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Sliding the Kernel:\n",
    "\n",
    "Now, we slide the kernel over the padded input array. For each position of the kernel, we take the region of the input that overlaps with the kernel, multiply each corresponding element of the kernel and the input, and sum the results.\n",
    "The result of each position is stored in the corresponding location in the output array."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
