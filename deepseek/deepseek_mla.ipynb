{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c7ff294",
   "metadata": {},
   "source": [
    "MLA computation using the steps as mentioned in figure:\n",
    "![mla](./mla.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a94af55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding X: torch.Size([6, 6])\n",
      "Q: torch.Size([6, 8])\n",
      "K: torch.Size([6, 8])\n",
      "V: torch.Size([6, 8])\n",
      "Attention Weights: torch.Size([6, 6])\n",
      "Context: torch.Size([6, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Dimensions\n",
    "seq_len = 6       # number of tokens\n",
    "embed_dim = 6     # token embedding dim\n",
    "d_kv = 4          # intermediate latent dim\n",
    "d_model = 8       # final Q, K, V dim\n",
    "\n",
    "# 1. Sample input (embedding matrix for 6 tokens)\n",
    "X = torch.randn(seq_len, embed_dim)  # shape: (6, 6)\n",
    "\n",
    "# 2. Define projection weights\n",
    "Wq   = torch.randn(embed_dim, d_model)    # (6, 8)\n",
    "Wdkv = torch.randn(embed_dim, d_kv)       # (6, 4)\n",
    "Wuk  = torch.randn(d_kv, d_model)         # (4, 8)\n",
    "Wuv  = torch.randn(d_kv, d_model)         # (4, 8)\n",
    "\n",
    "# 3. KV Caching: latent matrix (Ckv)\n",
    "Ckv = X @ Wdkv  # shape: (6, 4)\n",
    "\n",
    "# 4. Projections\n",
    "Q = X @ Wq         # (6, 8)\n",
    "K = Ckv @ Wuk      # (6, 8)\n",
    "V = Ckv @ Wuv      # (6, 8)\n",
    "\n",
    "# 5. Attention score computation\n",
    "attn_scores = (Q @ K.T) / (d_model ** 0.5)  # shape: (6, 6)\n",
    "\n",
    "# Optional: Apply causal mask (prevent attending to future)\n",
    "mask = torch.tril(torch.ones(seq_len, seq_len))  # lower triangular\n",
    "attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "# 6. Softmax to get attention weights\n",
    "attn_weights = F.softmax(attn_scores, dim=-1)  # shape: (6, 6)\n",
    "\n",
    "# 7. Context matrix (output of attention)\n",
    "context = attn_weights @ V  # shape: (6, 8)\n",
    "\n",
    "# Print shapes\n",
    "print(\"Embedding X:\", X.shape)\n",
    "print(\"Q:\", Q.shape)\n",
    "print(\"K:\", K.shape)\n",
    "print(\"V:\", V.shape)\n",
    "print(\"Attention Weights:\", attn_weights.shape)\n",
    "print(\"Context:\", context.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecadf624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked Attention score diff (orig vs opt): nan\n",
      "Masked Context diff (orig vs opt): 1.1026859283447266e-06\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Dimensions\n",
    "seq_len = 6\n",
    "embed_dim = 6\n",
    "d_kv = 4\n",
    "d_model = 8\n",
    "\n",
    "# Seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Sample input embeddings (X)\n",
    "X = torch.randn(seq_len, embed_dim)  # (6, 6)\n",
    "\n",
    "# Projection matrices\n",
    "Wq   = torch.randn(embed_dim, d_model)   # (6, 8)\n",
    "Wdkv = torch.randn(embed_dim, d_kv)      # (6, 4)\n",
    "Wuk  = torch.randn(d_kv, d_model)        # (4, 8)\n",
    "Wuv  = torch.randn(d_kv, d_model)        # (4, 8)\n",
    "\n",
    "# ========= Causal Mask =========\n",
    "mask = torch.tril(torch.ones(seq_len, seq_len))  # (6, 6)\n",
    "\n",
    "# ========== Original Computation ==========\n",
    "\n",
    "Q = X @ Wq               # (6, 8)\n",
    "Ckv = X @ Wdkv           # (6, 4)\n",
    "K = Ckv @ Wuk            # (6, 8)\n",
    "V = Ckv @ Wuv            # (6, 8)\n",
    "\n",
    "attn_scores_orig = (Q @ K.T) / (d_model ** 0.5)\n",
    "attn_scores_orig = attn_scores_orig.masked_fill(mask == 0, float('-inf'))\n",
    "attn_weights_orig = F.softmax(attn_scores_orig, dim=-1)\n",
    "context_orig = attn_weights_orig @ V\n",
    "\n",
    "# ========== Optimized Computation ==========\n",
    "\n",
    "# Precompute Wq @ Wuk^T\n",
    "fused_qk_proj = Wq @ Wuk.T  # (6, 4)\n",
    "\n",
    "attn_scores_opt = (X @ fused_qk_proj) @ Ckv.T\n",
    "attn_scores_opt = attn_scores_opt / (d_model ** 0.5)\n",
    "attn_scores_opt = attn_scores_opt.masked_fill(mask == 0, float('-inf'))\n",
    "attn_weights_opt = F.softmax(attn_scores_opt, dim=-1)\n",
    "context_opt = attn_weights_opt @ V\n",
    "\n",
    "# ========== Comparison ==========\n",
    "\n",
    "print(\"Masked Attention score diff (orig vs opt):\", torch.norm(attn_scores_orig - attn_scores_opt).item())\n",
    "print(\"Masked Context diff (orig vs opt):\", torch.norm(context_orig - context_opt).item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a247b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9969, -9.4262, -2.8623, -2.8189, 13.2914, -7.2141, 11.3604, -1.5934],\n",
       "        [-3.3808, -1.0989,  0.3626,  1.5451, -1.6427, -6.1897, -1.5837,  2.0744],\n",
       "        [-3.3803, -1.0985,  0.3625,  1.5446, -1.6424, -6.1883, -1.5834,  2.0739],\n",
       "        [-3.2030, -1.3739,  0.2528,  1.3568, -1.0646, -6.1085, -1.0487,  1.9079],\n",
       "        [-0.9969, -9.4262, -2.8623, -2.8189, 13.2914, -7.2141, 11.3604, -1.5934],\n",
       "        [-0.9964, -9.4248, -2.8617, -2.8185, 13.2895, -7.2130, 11.3591, -1.5931]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "417e071e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9969, -9.4262, -2.8623, -2.8189, 13.2914, -7.2141, 11.3604, -1.5934],\n",
       "        [-3.3808, -1.0989,  0.3626,  1.5451, -1.6427, -6.1897, -1.5837,  2.0744],\n",
       "        [-3.3803, -1.0985,  0.3625,  1.5446, -1.6424, -6.1883, -1.5834,  2.0739],\n",
       "        [-3.2030, -1.3739,  0.2528,  1.3568, -1.0646, -6.1085, -1.0487,  1.9079],\n",
       "        [-0.9969, -9.4262, -2.8623, -2.8189, 13.2914, -7.2141, 11.3604, -1.5934],\n",
       "        [-0.9964, -9.4248, -2.8617, -2.8185, 13.2895, -7.2130, 11.3591, -1.5931]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729af015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9269,  1.4873,  0.9007, -2.1055,  0.6784, -1.2345],\n",
       "        [-0.0431, -1.6047, -0.7521,  1.6487, -0.3925, -1.4036],\n",
       "        [-0.7279, -0.5594, -0.7688,  0.7624,  1.6423, -0.1596],\n",
       "        [-0.4974,  0.4396,  0.3189, -0.4245,  0.3057, -0.7746],\n",
       "        [ 0.0349,  0.3211,  1.5736, -0.8455, -1.2742,  2.1228]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:5] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03eed2c8",
   "metadata": {},
   "source": [
    "Compute the first 5 tokens first, then compute the 6th token\n",
    "\n",
    "![mla 2](./mla_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c42c8e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len_old = 5\n",
    "X_old = X[:seq_len_old]  # Use first 5 tokens for old computation\n",
    "\n",
    "Q = X_old @ Wq\n",
    "Ckv = X_old @ Wdkv\n",
    "K = Ckv @ Wuk\n",
    "V = Ckv @ Wuv\n",
    "\n",
    "mask = torch.tril(torch.ones(seq_len_old, seq_len_old)) \n",
    "\n",
    "attn_scores_orig = (Q @ K.T) / (d_model ** 0.5)\n",
    "attn_scores_orig = attn_scores_orig.masked_fill(mask == 0, float('-inf'))\n",
    "attn_weights_orig = F.softmax(attn_scores_orig, dim=-1)\n",
    "context_orig_old = attn_weights_orig @ V\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8f81716c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9964, -9.4248, -2.8617, -2.8185, 13.2895, -7.2130, 11.3591, -1.5931]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X[seq_len_old:]  # Use remaining tokens for new computation\n",
    "Ckv_new = X_new @ Wdkv  # (1, 4)\n",
    "\n",
    "# append new Ckv to existing Ckv\n",
    "Ckv_combined = torch.cat((Ckv, Ckv_new), dim=0)\n",
    "\n",
    "attn_score_new = X_new @ fused_qk_proj @ Ckv_combined.T / (d_model ** 0.5)\n",
    "attn_weights_new = F.softmax(attn_score_new , dim=-1)\n",
    "\n",
    "V_combined = Ckv_combined @ Wuv  \n",
    "\n",
    "context_new = attn_weights_new @ V_combined\n",
    "\n",
    "context_new\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
