{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "522fcfb7",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=W7ktPe1HfZs&list=PLPTV0NXA_ZSiOpKKlHCyOq9lnp-dLvlms&index=22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c51fdec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1bae95d4e70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0c578b",
   "metadata": {},
   "source": [
    "Note: wget does not work in the VS Code terminal, so we use curl instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd821e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/Avisoori1x/makeMoE/main/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62c0e950",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      " 71 1089k   71  780k    0     0   547k      0  0:00:01  0:00:01 --:--:--  548k\n",
      "100 1089k  100 1089k    0     0   741k      0  0:00:01  0:00:01 --:--:--  742k\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://raw.githubusercontent.com/Avisoori1x/makeMoE/main/input.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93088c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4 * n_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embed, n_embed)\n",
    "            nn.Dropout(dropout),  # Add dropout for regularization\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee459b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 4, 3])\n",
      "Logits: tensor([[[-1.5546, -0.2941,  1.3507],\n",
      "         [-0.2603,  0.2799,  0.7907],\n",
      "         [-0.9108,  0.3964,  0.4110],\n",
      "         [-0.2563, -0.4846, -0.7813]],\n",
      "\n",
      "        [[ 0.4986,  0.4953, -0.3828],\n",
      "         [ 0.0560,  0.7386, -0.6886],\n",
      "         [-0.5285, -0.3301,  1.5522],\n",
      "         [-0.3325,  0.4946,  0.2142]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "num_experts = 3\n",
    "top_k = 2\n",
    "n_embed = 8\n",
    "\n",
    "mh_output = torch.randn(2, 4, n_embed)\n",
    "\n",
    "# routing matrix\n",
    "topkgate_linear = nn.Linear(n_embed, num_experts)\n",
    "\n",
    "logits = topkgate_linear(mh_output)\n",
    "\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "print(\"Logits:\", logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd45e19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-k logits: tensor([[[ 1.3507, -0.2941],\n",
      "         [ 0.7907,  0.2799],\n",
      "         [ 0.4110,  0.3964],\n",
      "         [-0.2563, -0.4846]],\n",
      "\n",
      "        [[ 0.4986,  0.4953],\n",
      "         [ 0.7386,  0.0560],\n",
      "         [ 1.5522, -0.3301],\n",
      "         [ 0.4946,  0.2142]]], grad_fn=<TopkBackward0>)\n",
      "Top-k indices: tensor([[[2, 1],\n",
      "         [2, 1],\n",
      "         [2, 1],\n",
      "         [0, 1]],\n",
      "\n",
      "        [[0, 1],\n",
      "         [1, 0],\n",
      "         [2, 1],\n",
      "         [1, 2]]])\n"
     ]
    }
   ],
   "source": [
    "top_k_logits, top_k_indices = logits.topk(top_k, dim=-1)\n",
    "\n",
    "print(\"Top-k logits:\", top_k_logits)\n",
    "print(\"Top-k indices:\", top_k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58370213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeros: tensor([[[-inf, -inf, -inf],\n",
      "         [-inf, -inf, -inf],\n",
      "         [-inf, -inf, -inf],\n",
      "         [-inf, -inf, -inf]],\n",
      "\n",
      "        [[-inf, -inf, -inf],\n",
      "         [-inf, -inf, -inf],\n",
      "         [-inf, -inf, -inf],\n",
      "         [-inf, -inf, -inf]]])\n",
      "Sparse logits: tensor([[[   -inf, -0.2941,  1.3507],\n",
      "         [   -inf,  0.2799,  0.7907],\n",
      "         [   -inf,  0.3964,  0.4110],\n",
      "         [-0.2563, -0.4846,    -inf]],\n",
      "\n",
      "        [[ 0.4986,  0.4953,    -inf],\n",
      "         [ 0.0560,  0.7386,    -inf],\n",
      "         [   -inf, -0.3301,  1.5522],\n",
      "         [   -inf,  0.4946,  0.2142]]], grad_fn=<ScatterBackward0>)\n"
     ]
    }
   ],
   "source": [
    "zeros = torch.full_like(logits, fill_value=float(\"-inf\"))\n",
    "\n",
    "print(\"Zeros:\", zeros)\n",
    "\n",
    "sparse_logits = zeros.scatter(\n",
    "    dim=-1,\n",
    "    index=top_k_indices,\n",
    "    src=top_k_logits\n",
    ")\n",
    "\n",
    "print(\"Sparse logits:\", sparse_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9a9c574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gating output: tensor([[[0.0000, 0.1618, 0.8382],\n",
      "         [0.0000, 0.3750, 0.6250],\n",
      "         [0.0000, 0.4963, 0.5037],\n",
      "         [0.5568, 0.4432, 0.0000]],\n",
      "\n",
      "        [[0.5008, 0.4992, 0.0000],\n",
      "         [0.3357, 0.6643, 0.0000],\n",
      "         [0.0000, 0.1321, 0.8679],\n",
      "         [0.0000, 0.5696, 0.4304]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "gating_output = F.softmax(sparse_logits, dim=-1)\n",
    "\n",
    "print(\"Gating output:\", gating_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4561b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopkRouter(nn.Module):\n",
    "    def __init__(self, m_embed, num_experts, top_k):\n",
    "        super().__init__()        \n",
    "        self.top_k = top_k\n",
    "        self.linear = nn.Linear(m_embed, num_experts)\n",
    "    \n",
    "    def forward(self, mh_output):\n",
    "        logits = self.linear(mh_output)\n",
    "        top_k_logits, top_k_indices = logits.topk(self.top_k, dim=-1)\n",
    "        zeros = torch.full_like(logits, fill_value=float(\"-inf\"))\n",
    "        sparse_logits = zeros.scatter(\n",
    "            dim=-1,\n",
    "            index=top_k_indices,\n",
    "            src=top_k_logits\n",
    "        )\n",
    "        gating_output = F.softmax(sparse_logits, dim=-1)\n",
    "        return gating_output, top_k_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4f90ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gating output shape: torch.Size([1, 4, 3])\n",
      "Gating output: tensor([[[0.0000, 0.5124, 0.4876],\n",
      "         [0.5763, 0.0000, 0.4237],\n",
      "         [0.6128, 0.0000, 0.3872],\n",
      "         [0.4348, 0.0000, 0.5652]]], grad_fn=<SoftmaxBackward0>)\n",
      "Top-k indices shape: torch.Size([1, 4, 2])\n",
      "Top-k indices: tensor([[[1, 2],\n",
      "         [0, 2],\n",
      "         [0, 2],\n",
      "         [2, 0]]])\n"
     ]
    }
   ],
   "source": [
    "# test the TopkRouter\n",
    "\n",
    "num_experts = 3\n",
    "top_k = 2\n",
    "m_embed = 8\n",
    "\n",
    "mh_output = torch.randn(1, 4, m_embed)\n",
    "\n",
    "router = TopkRouter(m_embed, num_experts, top_k)\n",
    "\n",
    "gating_output, top_k_indices = router(mh_output)\n",
    "\n",
    "print(\"Gating output shape:\", gating_output.shape)\n",
    "print(\"Gating output:\", gating_output)\n",
    "print(\"Top-k indices shape:\", top_k_indices.shape)\n",
    "print(\"Top-k indices:\", top_k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5cc3f2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyTopkRouter(nn.Module):\n",
    "    def __init__(self, m_embed, num_experts, top_k):\n",
    "        super().__init__()\n",
    "        self.top_k = top_k\n",
    "        self.linear = nn.Linear(m_embed, num_experts)\n",
    "        self.noise_linear = nn.Linear(m_embed, num_experts)\n",
    "\n",
    "    def forward(self, mh_output):\n",
    "        logits = self.linear(mh_output)\n",
    "\n",
    "        noise_logits = self.noise_linear(mh_output)\n",
    "\n",
    "        noise = torch.randn_like(logits) * F.softplus(noise_logits)\n",
    "        noisy_logits = logits + noise\n",
    "\n",
    "        top_k_logits, top_k_indices = noisy_logits.topk(self.top_k, dim=-1)\n",
    "        zeros = torch.full_like(logits, fill_value=float(\"-inf\"))\n",
    "        sparse_logits = zeros.scatter(\n",
    "            dim=-1,\n",
    "            index=top_k_indices,\n",
    "            src=top_k_logits\n",
    "        )\n",
    "        gating_output = F.softmax(sparse_logits, dim=-1)\n",
    "        return gating_output, top_k_indices\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbcb401f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gating output shape: torch.Size([1, 4, 3])\n",
      "Gating output: tensor([[[0.9068, 0.0000, 0.0932],\n",
      "         [0.5094, 0.4906, 0.0000],\n",
      "         [0.0000, 0.5448, 0.4552],\n",
      "         [0.5408, 0.4592, 0.0000]]], grad_fn=<SoftmaxBackward0>)\n",
      "Top-k indices shape: torch.Size([1, 4, 2])\n",
      "Top-k indices: tensor([[[0, 2],\n",
      "         [0, 1],\n",
      "         [1, 2],\n",
      "         [0, 1]]])\n"
     ]
    }
   ],
   "source": [
    "# test the NoisyTopkRouter\n",
    "\n",
    "num_experts = 3\n",
    "top_k = 2\n",
    "m_embed = 8\n",
    "\n",
    "mh_output = torch.randn(1, 4, m_embed)\n",
    "\n",
    "router = NoisyTopkRouter(m_embed, num_experts, top_k)\n",
    "\n",
    "gating_output, top_k_indices = router(mh_output)\n",
    "\n",
    "print(\"Gating output shape:\", gating_output.shape)\n",
    "print(\"Gating output:\", gating_output)\n",
    "print(\"Top-k indices shape:\", top_k_indices.shape)\n",
    "print(\"Top-k indices:\", top_k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0b0c8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
