{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generted by chatgpt: https://chatgpt.com/share/6711cd9e-4498-8004-b06f-8841f4bfba65\n",
    "\n",
    "torch.multinomial is a function that samples from a probability distribution, but it doesn't always select the index with the maximum probability. Instead, it picks an index based on the probability distribution you provide, making it useful for generating stochastic results.\n",
    "\n",
    "How torch.multinomial Works:\n",
    "- Input: A tensor of probabilities (often generated by torch.softmax).\n",
    "\n",
    "- Behavior: It samples indices according to the probabilities. Higher probabilities mean the corresponding indices are more likely to be chosen, but even indices with lower probabilities have a chance of being selected (though the likelihood decreases with the probability).\n",
    "\n",
    "It works similarly to a weighted lottery:\n",
    "\n",
    "  - If an index has a probability of 0.7, it has a 70% chance of being selected.\n",
    "  - An index with a probability of 0.1 has a 10% chance, and so on.\n",
    "\n",
    "Key Points:\n",
    "1. Sampling Behavior:\n",
    "\n",
    "  - torch.multinomial is probabilistic, meaning it samples randomly based on the distribution. This does not necessarily return the index with the highest probability.\n",
    "  - If you want the index with the highest probability (i.e., deterministic selection), you would use torch.argmax.\n",
    "\n",
    "2. Number of Samples:\n",
    "\n",
    "  - You can specify how many samples you want to draw from the probability distribution. For instance, torch.multinomial(probs, 1) samples one index, but you could sample multiple indices (with or without replacement).\n",
    "\n",
    "3. With or Without Replacement:\n",
    "\n",
    "  - With replacement (default): The same index can be picked multiple times.\n",
    "  - Without replacement: Once an index is selected, it can't be selected again in the same sampling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example probability distribution\n",
    "probs = torch.tensor([0.1, 0.2, 0.3, 0.4])\n",
    "\n",
    "# Sampling one index based on the probabilities\n",
    "sample = torch.multinomial(probs, 1)\n",
    "print(sample)  # Output could be any index, but index 3 has the highest chance\n",
    "\n",
    "# If you want the index with the highest probability\n",
    "max_index = torch.argmax(probs)\n",
    "print(max_index)  # Output: tensor(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Difference Between torch.multinomial and torch.argmax:\n",
    "- torch.multinomial: Stochastic sampling based on probabilities.\n",
    "  - Indexes are selected randomly according to the distribution.\n",
    "  - Useful for tasks where exploration or diversity in outputs is important (e.g., text generation).\n",
    "\n",
    "- torch.argmax: Deterministic selection.\n",
    "  - Always picks the index with the highest probability.\n",
    "  - Useful for tasks where you want the \"most likely\" or \"best\" option, like classification tasks.\n",
    "\n",
    "Use Cases:\n",
    "- torch.multinomial: Commonly used in models that need to sample outputs probabilistically (e.g., during training of language models, text generation, etc.).\n",
    "- torch.argmax: Used for tasks like classification, where you want the most probable or confident answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"heat\" or \"temperature\" parameter in large language models (LLMs) is closely related to the way probabilities are handled when generating output. It affects the distribution of probabilities and how random the sampling process is, influencing the behavior of functions like torch.softmax and torch.multinomial.\n",
    "\n",
    "How Temperature (or Heat) Works:\n",
    "In the context of LLMs, temperature controls the randomness or \"creativity\" of the output. It is applied before sampling from the probabilities, typically by modifying the logits before applying the softmax function. The temperature influences the shape of the probability distribution over possible outputs.\n",
    "\n",
    "Mathematical Effect:\n",
    "Given a logits tensor (the raw output from the model), the temperature is applied as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.softmax(logits / temperature, dim=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When temperature is high (e.g., temperature > 1): The logits are divided by a large value, making the differences between logits smaller, flattening the probability distribution. This means more random sampling because the probabilities of all tokens become more equal.\n",
    "\n",
    "- When temperature is low (e.g., temperature < 1): The logits are divided by a small value, which amplifies the differences between them. This leads to a sharper or more peaked distribution where the highest-probability tokens are more likely to be picked (less randomness).\n",
    "\n",
    "Behavior at Extreme Values:\n",
    "\n",
    "1. temperature = 1:\n",
    "\n",
    "  - The probabilities are left unchanged.\n",
    "  - The model will behave as expected, using the unmodified logits for sampling.\n",
    "\n",
    "2. temperature < 1:\n",
    "\n",
    "  - The model becomes more deterministic.\n",
    "  - The probability distribution sharpens, making the model more confident in high-probability tokens. With very low temperatures, it approaches behavior like torch.argmax, where the model always selects the most likely next token.\n",
    "  \n",
    "3. temperature > 1:\n",
    "\n",
    "  - The model becomes more random.\n",
    "  - The probability distribution flattens, reducing the difference between high-probability and low-probability tokens. This increases randomness, and even less likely tokens have a chance of being sampled.\n",
    "\n",
    "4. temperature = 0:\n",
    "\n",
    "  - This is equivalent to torch.argmax, where the token with the maximum probability is always selected, i.e., fully deterministic output.\n",
    "\n",
    "Connection to torch.softmax and torch.multinomial:\n",
    "\n",
    "1. Softmax with Temperature:\n",
    "\n",
    "  - The temperature modifies the logits before the softmax is applied, which changes the distribution of probabilities that are then fed into torch.multinomial for sampling.\n",
    "\n",
    "  - Lower temperatures make the distribution more \"peaked,\" meaning torch.multinomial will more often select the highest-probability tokens.\n",
    "  \n",
    "  - Higher temperatures make the distribution more \"flat,\" meaning torch.multinomial will select from a wider range of tokens.\n",
    "\n",
    "2. torch.multinomial:\n",
    "\n",
    "  - This function samples based on the probabilities generated by softmax. If the temperature is high (leading to flatter probabilities), it increases the randomness of the sampling process. If the temperature is low, the output of torch.multinomial will be more focused on the highest-probability tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities with temperature: tensor([0.1298, 0.5416, 0.0635, 0.2651])\n",
      "Sampled token: tensor([0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Logits from a language model, for example\n",
    "logits = torch.tensor([1.0, 2.0, 0.5, 1.5])\n",
    "\n",
    "# Apply temperature (e.g., 0.7 for lower randomness, 1.5 for higher randomness)\n",
    "temperature = 0.7\n",
    "adjusted_logits = logits / temperature\n",
    "\n",
    "# Softmax to get probabilities\n",
    "probs = torch.softmax(adjusted_logits, dim=-1)\n",
    "print(\"Probabilities with temperature:\", probs)\n",
    "\n",
    "# Sample from the distribution\n",
    "sample = torch.multinomial(probs, 1)\n",
    "print(\"Sampled token:\", sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary:\n",
    "- Temperature (or heat) adjusts the randomness of token generation in LLMs.\n",
    "- It modifies the logits before applying torch.softmax, impacting the probability distribution.\n",
    "- Lower temperatures (closer to 0) make the output more deterministic, similar to using torch.argmax.\n",
    "- Higher temperatures make the output more random, influencing how torch.multinomial samples tokens."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
