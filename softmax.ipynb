{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://chatgpt.com/share/6710cef0-a2e0-8004-a321-5a932b64df44\n",
    "\n",
    "The term **softmax** comes from the fact that this function is a \"softened\" or \"smooth\" version of the **argmax** function.\n",
    "\n",
    "Here's a breakdown of the reasoning:\n",
    "\n",
    "### 1. **Argmax** Function:\n",
    "The **argmax** function returns the index of the largest value in a set of numbers. It is a hard selection, meaning it gives all the \"weight\" or importance to the largest value and completely ignores the rest. For example:\n",
    "\n",
    "```python\n",
    "input = [1.2, 2.5, 0.9]\n",
    "argmax(input) = 1  # Index of the largest value (2.5)\n",
    "```\n",
    "\n",
    "In this case, the function simply returns which number is the largest, discarding any information about how close the other values are.\n",
    "\n",
    "### 2. **Softmax** Function:\n",
    "The **softmax** function, on the other hand, doesn't just pick the largest value. Instead, it assigns a probability to each value in the set, effectively creating a probability distribution over all the inputs. It does this by exponentiating the values and normalizing them so that they sum to 1:\n",
    "\n",
    "\\[\n",
    "\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}\n",
    "\\]\n",
    "\n",
    "The result is a set of probabilities where larger values get more \"weight\" (closer to 1), but smaller values still contribute (closer to 0). It \"softens\" the decision by spreading the probabilities across all values rather than making a hard choice like argmax.\n",
    "\n",
    "For example:\n",
    "\n",
    "```python\n",
    "input = [1.2, 2.5, 0.9]\n",
    "softmax(input) ≈ [0.186, 0.689, 0.125]\n",
    "```\n",
    "\n",
    "Here, the second value (2.5) gets the highest probability (0.689), but the other values still get non-zero probabilities (0.186 and 0.125), unlike argmax, which would have given all the importance to the largest value.\n",
    "\n",
    "### Why \"Soft\" Max?\n",
    "- The \"max\" part refers to the fact that softmax emphasizes the largest values.\n",
    "- The \"soft\" part means that instead of making a hard decision (like argmax), it provides a probabilistic interpretation where all values contribute to the final outcome.\n",
    "\n",
    "This makes the softmax function especially useful in machine learning (e.g., classification tasks), where it turns raw logits or scores into probabilities that can be interpreted as likelihoods for different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training:\n",
    "logits -> cross_entropy (log_softmax -> nll_loss)\n",
    "\n",
    "inference:\n",
    "logits -> softmax -> multinomial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "logits = torch.rand(2,4,8)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0948, 0.0774, 0.1019, 0.2011, 0.1612, 0.1579, 0.1310, 0.0748],\n",
      "         [0.0683, 0.1557, 0.1600, 0.1581, 0.1141, 0.1068, 0.1239, 0.1131],\n",
      "         [0.1489, 0.1089, 0.0765, 0.1598, 0.1677, 0.1713, 0.0731, 0.0937],\n",
      "         [0.0777, 0.0900, 0.1416, 0.1632, 0.1730, 0.0929, 0.1693, 0.0923]],\n",
      "\n",
      "        [[0.1309, 0.1773, 0.1718, 0.0777, 0.0784, 0.1623, 0.1136, 0.0879],\n",
      "         [0.0849, 0.0853, 0.1853, 0.1622, 0.1035, 0.1123, 0.1594, 0.1071],\n",
      "         [0.0701, 0.1737, 0.1315, 0.1088, 0.1616, 0.0688, 0.1329, 0.1525],\n",
      "         [0.0798, 0.1359, 0.1405, 0.1493, 0.1295, 0.0905, 0.1571, 0.1173]]])\n"
     ]
    }
   ],
   "source": [
    "probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n",
      "tensor([[1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "sum = torch.sum(probs, dim=-1)\n",
    "print(sum.shape)\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term **logits** comes from **log-odds**, which is related to how logits are used in machine learning, particularly in classification tasks.\n",
    "\n",
    "### Breaking Down the Name \"Logits\":\n",
    "\n",
    "1. **Log-Odds**:\n",
    "   - The **odds** of an event happening are the ratio of the probability of the event happening to the probability of it not happening.\n",
    "     \\[\n",
    "     \\text{Odds}(p) = \\frac{p}{1 - p}\n",
    "     \\]\n",
    "   - The **log-odds** (or **logit**) is simply the logarithm of the odds:\n",
    "     \\[\n",
    "     \\text{log-odds}(p) = \\log\\left(\\frac{p}{1 - p}\\right)\n",
    "     \\]\n",
    "   - This transforms a probability (which is between 0 and 1) into a value that can range from \\(-\\infty\\) to \\(+\\infty\\).\n",
    "\n",
    "2. **Logits in Machine Learning**:\n",
    "   - In the context of machine learning, **logits** refer to the unnormalized scores that are often passed to a softmax function for multi-class classification. These logits represent the model's confidence in each class before being converted into probabilities.\n",
    "   - The idea is that logits can be interpreted similarly to **log-odds**, though they aren’t exactly log-odds in most cases. They are raw scores that, when passed through softmax, give the equivalent of probabilities for each class.\n",
    "   - In binary classification using logistic regression, the raw output is indeed the log-odds (hence the term **logit**), which is then converted to a probability using the **sigmoid** function. For multi-class classification, the softmax function generalizes this concept.\n",
    "\n",
    "3. **Logistic Function**:\n",
    "   - The **logistic function** (used in logistic regression) converts log-odds into probabilities:\n",
    "     \\[\n",
    "     \\text{sigmoid}(z) = \\frac{1}{1 + e^{-z}}\n",
    "     \\]\n",
    "     This function maps a logit (which can be any real number) into a probability between 0 and 1.\n",
    "   - In **binary classification**, the output of a model is often interpreted as log-odds, and the logistic function converts those into probabilities, making the term **logit** a natural name.\n",
    "\n",
    "### Why \"Logits\" in Modern Deep Learning:\n",
    "- While in deep learning (especially with softmax), logits are not directly the log-odds, the term is still used to refer to the **raw, unnormalized scores** that precede the probability computation.\n",
    "- The name **logits** stuck because it captures the idea that these are intermediate values that can be transformed into probabilities through a suitable function (like softmax or sigmoid).\n",
    "\n",
    "### Summary:\n",
    "The name **logits** comes from the concept of **log-odds** in statistics. Even though modern logits are not always log-odds, the term is used to describe the unnormalized scores output by models before they are transformed into probabilities using functions like softmax or sigmoid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To explain how torch.nn.functional.nll_loss works, let's walk through a simple example with some sample data.\n",
    "\n",
    "Key Points about nll_loss:\n",
    "- NLL Loss (Negative Log Likelihood Loss) is often used in classification problems where the model outputs log probabilities (often the output from log_softmax).\n",
    "- The function compares the log-probabilities from the model to the actual target labels.\n",
    "- This loss function expects the input to be log probabilities and the target to be class indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLL Loss: 0.4750000238418579\n",
      "Manual NLL Loss: 0.475\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Log-probabilities for 4 samples and 3 classes (after applying log_softmax)\n",
    "log_probs = torch.tensor([\n",
    "    [-0.5, -1.0, -2.0],  # Sample 1\n",
    "    [-0.1, -2.0, -0.9],  # Sample 2\n",
    "    [-1.5, -0.2, -1.3],  # Sample 3\n",
    "    [-0.3, -0.8, -0.5]   # Sample 4\n",
    "])\n",
    "\n",
    "# Target labels for each sample (true class indices)\n",
    "targets = torch.tensor([0, 2, 1, 0])\n",
    "\n",
    "# Calculate NLL Loss\n",
    "loss = F.nll_loss(log_probs, targets)\n",
    "\n",
    "print(f\"NLL Loss: {loss.item()}\")\n",
    "\n",
    "# manual calculation\n",
    "manual_loss = -((-0.5) + (-0.9) + (-0.2) + (-0.3)) / 4\n",
    "print(f\"Manual NLL Loss: {manual_loss}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3743)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Instantiate the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Sample logits and labels\n",
    "logits = torch.tensor([[1.0, 2.0, 0.1],\n",
    "                       [1.2, 0.5, 0.3],\n",
    "                       [0.4, 1.0, 1.5]], dtype=torch.float32)\n",
    "labels = torch.tensor([2, 0, 1], dtype=torch.long)\n",
    "\n",
    "# Call the instantiated object to compute the loss\n",
    "loss = criterion(logits, labels)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2424, 0.6590, 0.0986],\n",
      "        [0.5254, 0.2609, 0.2136],\n",
      "        [0.1716, 0.3127, 0.5156]])\n",
      "tensor([[-1.4170, -0.4170, -2.3170],\n",
      "        [-0.6435, -1.3435, -1.5435],\n",
      "        [-1.7624, -1.1624, -0.6624]])\n",
      "incorrect_nll:  tensor(-0.3123)\n",
      "correct_nll: tensor(1.3743)\n",
      "cross entropy:  tensor(1.3743)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "logits = torch.tensor([[1.0, 2.0, 0.1],\n",
    "                       [1.2, 0.5, 0.3],\n",
    "                       [0.4, 1.0, 1.5]], dtype=torch.float32)\n",
    "\n",
    "sm = F.softmax(logits, dim=-1)\n",
    "print(sm)\n",
    "\n",
    "lsm = F.log_softmax(logits, dim=-1)\n",
    "print(lsm)\n",
    "\n",
    "labels = torch.tensor([2, 0, 1], dtype=torch.long)\n",
    "\n",
    "# incorrect\n",
    "incorrect_nll = F.nll_loss(sm, labels)\n",
    "\n",
    "# correct, we should use log_softmax\n",
    "correct_nll = F.nll_loss(lsm, labels)\n",
    "\n",
    "print(\"incorrect_nll: \", incorrect_nll)\n",
    "print(\"correct_nll:\", correct_nll)\n",
    "\n",
    "cross_entropy = F.cross_entropy(logits, labels)\n",
    "print(\"cross entropy: \", cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.3122)\n",
      "tensor(1.3743)\n",
      "1.3743\n"
     ]
    }
   ],
   "source": [
    "# softmax\n",
    "t1 = torch.tensor([[0.2424, 0.6590, 0.0986],\n",
    "                   [0.5254, 0.2609, 0.2136],\n",
    "                   [0.1716, 0.3127, 0.5156]])\n",
    "\n",
    "# log_softmax\n",
    "t2 = torch.tensor([[-1.4170, -0.4170, -2.3170],\n",
    "                   [-0.6435, -1.3435, -1.5435],\n",
    "                   [-1.7624, -1.1624, -0.6624]])\n",
    "\n",
    "labels = torch.tensor([2, 0, 1], dtype=torch.long)\n",
    "\n",
    "nll1 = F.nll_loss(t1, labels)\n",
    "nll2 = F.nll_loss(t2, labels)\n",
    "\n",
    "print(nll1)\n",
    "print(nll2)\n",
    "\n",
    "# manual calculation based on log_softmax\n",
    "manual_loss1 = -((-2.3170) + (-0.6435) + (-1.1624)) / 3\n",
    "print(manual_loss1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4172, -0.4170, -2.3167],\n",
      "        [-0.6436, -1.3436, -1.5437],\n",
      "        [-1.7626, -1.1625, -0.6624]])\n"
     ]
    }
   ],
   "source": [
    "# manual log on softmax\n",
    "log_t1 = torch.log(t1)\n",
    "print(log_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
