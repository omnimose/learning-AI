{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sigmoid vs Softmax: Key Differences**\n",
    "\n",
    "| Feature        | **Sigmoid** | **Softmax** |\n",
    "|--------------|------------|------------|\n",
    "| **Definition** | A function that maps input values to a range between 0 and 1. | A function that converts input values into a probability distribution. |\n",
    "| **Formula** | $ \\sigma(x) = \\frac{1}{1 + e^{-x}} $ | $ \\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}} $ |\n",
    "| **Output Range** | (0, 1) for each value independently. | (0, 1) for each value, with all outputs summing to 1. |\n",
    "| **Use Case** | Used for binary classification (single output neuron). | Used for multi-class classification (multiple output neurons). |\n",
    "| **Independence** | Each output is independent. | Outputs are interdependent (they sum to 1). |\n",
    "| **Interpretation** | Output can be interpreted as a probability but does not ensure mutual exclusivity. | Outputs can be directly interpreted as probabilities of different classes. |\n",
    "| **Gradient Behavior** | Can suffer from vanishing gradients for extreme values. | Ensures a clear distinction between different classes but can be sensitive to large input values. |\n",
    "\n",
    "### **When to Use Each?**\n",
    "- **Sigmoid**: Best for binary classification tasks (e.g., \"Yes\" or \"No\" problems).\n",
    "- **Softmax**: Best for multi-class classification (e.g., distinguishing between multiple categories)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
