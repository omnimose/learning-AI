{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://medium.com/towards-data-science/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a\n",
    "- https://www.youtube.com/results?search_query=binary+cross+entropy+loss+explained\n",
    "\n",
    "- https://www.youtube.com/watch?v=Md4b67HvmRo&t=102s\n",
    "\n",
    "- https://cs231n.github.io/neural-networks-2/#losses\n",
    "\n",
    "chatgpt:\n",
    "- https://chatgpt.com/share/67b8221f-31ac-8004-836b-44d8bbbc0991"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Formula of Binary Cross Entropy (BCE) Loss:\n",
    "\n",
    "The formula for binary cross-entropy (logistic regression loss) is:\n",
    "\n",
    "$$\n",
    "L_i = - y \\log(\\sigma(f)) - (1 - y) \\log(1 - \\sigma(f))\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ L_i $ is the loss for a single training example $ i $.\n",
    "- $ y $ is the true label of the example (either 0 or 1).\n",
    "- $ \\sigma(f) $ is the predicted probability (output of the sigmoid function), where $ f = w^T x + b $ is the raw model output (the logit).\n",
    "  \n",
    "### Explanation:\n",
    "\n",
    "- **For a True Positive (y = 1):**  \n",
    "  When the true label $ y = 1 $, the loss becomes:\n",
    "  \n",
    "  $$\n",
    "  L_i = - \\log(\\sigma(f))\n",
    "  $$\n",
    "  \n",
    "  This is because $ (1 - y) $ becomes zero, so the second term vanishes. The model wants to maximize the probability of the true class being 1. The closer $ \\sigma(f) $ is to 1, the smaller the loss.\n",
    "  \n",
    "- **For a True Negative (y = 0):**  \n",
    "  When the true label $ y = 0 $, the loss becomes:\n",
    "  \n",
    "  $$\n",
    "  L_i = - \\log(1 - \\sigma(f))\n",
    "  $$\n",
    "  \n",
    "  This is because $ y $ becomes zero, so the first term vanishes. The model wants to maximize the probability of the true class being 0. The closer $ \\sigma(f) $ is to 0, the smaller the loss.\n",
    "\n",
    "### Example Walkthrough:\n",
    "\n",
    "Letâ€™s say we have the following:\n",
    "\n",
    "- **True label** $ y = 1 $ (positive class).\n",
    "- **Logit (raw model output)** $ f = 1.5 $ (this is the value $ w^T x + b $).\n",
    "- **Predicted probability** $ \\sigma(f) = \\sigma(1.5) = \\frac{1}{1 + e^{-1.5}} \\approx 0.817 $.\n",
    "\n",
    "We want to compute the loss for this example.\n",
    "\n",
    "#### Step 1: Apply the sigmoid function\n",
    "\n",
    "The sigmoid function is given by:\n",
    "\n",
    "$$\n",
    "\\sigma(f) = \\frac{1}{1 + e^{-f}}\n",
    "$$\n",
    "\n",
    "For $ f = 1.5 $:\n",
    "\n",
    "$$\n",
    "\\sigma(1.5) = \\frac{1}{1 + e^{-1.5}} \\approx \\frac{1}{1 + 0.223} \\approx 0.817\n",
    "$$\n",
    "\n",
    "So, the predicted probability for class 1 is approximately **0.817**.\n",
    "\n",
    "#### Step 2: Compute the loss for $ y = 1 $\n",
    "\n",
    "Since $ y = 1 $, the formula simplifies to:\n",
    "\n",
    "$$\n",
    "L_i = - \\log(\\sigma(f)) = - \\log(0.817)\n",
    "$$\n",
    "\n",
    "We calculate $ \\log(0.817) $:\n",
    "\n",
    "$$\n",
    "\\log(0.817) \\approx -0.202\n",
    "$$\n",
    "\n",
    "Thus, the loss for this example is:\n",
    "\n",
    "$$\n",
    "L_i = -(-0.202) = 0.202\n",
    "$$\n",
    "\n",
    "### Interpretation of the Loss:\n",
    "\n",
    "- The loss $ L_i = 0.202 $ means that the model predicted a probability of 0.817 for the true class (class 1), which is fairly close to the correct value of 1.\n",
    "- A lower loss would indicate a better prediction (closer to 1 for class 1), while a higher loss would indicate a poorer prediction.\n",
    "\n",
    "### Why This Loss is Important:\n",
    "\n",
    "- **Minimizing the loss**: The goal of training is to adjust the model parameters such that the predicted probability for the true class (class 1) is as high as possible for positive examples ($ y = 1 $), and as low as possible for negative examples ($ y = 0 $).\n",
    "- The **logarithmic nature** of the loss function means that large mistakes (i.e., predicting a very low probability for class 1 when $ y = 1 $) result in a much higher loss than smaller mistakes.\n",
    "\n",
    "For example:\n",
    "- If $ \\sigma(f) $ were 0.1 instead of 0.817, the loss would be:\n",
    "\n",
    "$$\n",
    "L_i = - \\log(0.1) \\approx 2.302\n",
    "$$\n",
    "\n",
    "This would be a much higher loss, indicating a worse prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary Cross Entropy (BCE) Loss sample calculation\n",
    "\n",
    "**cross-entropy loss** is computed for logistic regression. We will use the formula:\n",
    "\n",
    "$$\n",
    "L_i = - y \\log(\\sigma(f)) - (1 - y) \\log(1 - \\sigma(f))\n",
    "$$\n",
    "\n",
    "### Scenario 1: True Label $ y = 1 $ (positive class)\n",
    "\n",
    "**Sample Data:**\n",
    "- **True label $ y = 1 $**\n",
    "- **Raw score (logit) $ f = 2.0 $**\n",
    "- **We want to calculate the predicted probability $ \\sigma(f) $ using the sigmoid function.**\n",
    "\n",
    "#### Step 1: Apply the sigmoid function\n",
    "\n",
    "The sigmoid function is:\n",
    "\n",
    "$$\n",
    "\\sigma(f) = \\frac{1}{1 + e^{-f}}\n",
    "$$\n",
    "\n",
    "For $ f = 2.0 $:\n",
    "\n",
    "$$\n",
    "\\sigma(2.0) = \\frac{1}{1 + e^{-2.0}} = \\frac{1}{1 + 0.1353} \\approx 0.881\n",
    "$$\n",
    "\n",
    "So, the predicted probability for class 1 is **0.881**.\n",
    "\n",
    "#### Step 2: Calculate the loss\n",
    "\n",
    "Since the true label $ y = 1 $, the formula simplifies to:\n",
    "\n",
    "$$\n",
    "L_i = - \\log(\\sigma(f)) = - \\log(0.881)\n",
    "$$\n",
    "\n",
    "Now, we calculate:\n",
    "\n",
    "$$\n",
    "\\log(0.881) \\approx -0.127\n",
    "$$\n",
    "\n",
    "Thus, the loss for this example is:\n",
    "\n",
    "$$\n",
    "L_i = -(-0.127) = 0.127\n",
    "$$\n",
    "\n",
    "So, the loss for this example when the true label is **1** is **0.127**.\n",
    "\n",
    "---\n",
    "\n",
    "### Scenario 2: True Label $ y = 0 $ (negative class)\n",
    "\n",
    "**Sample Data:**\n",
    "- **True label $ y = 0 $**\n",
    "- **Raw score (logit) $ f = -2.0 $**\n",
    "- **We want to calculate the predicted probability $ \\sigma(f) $ using the sigmoid function.**\n",
    "\n",
    "#### Step 1: Apply the sigmoid function\n",
    "\n",
    "For $ f = -2.0 $:\n",
    "\n",
    "$$\n",
    "\\sigma(-2.0) = \\frac{1}{1 + e^{2.0}} = \\frac{1}{1 + 7.389} \\approx 0.119\n",
    "$$\n",
    "\n",
    "So, the predicted probability for class 1 is **0.119**.\n",
    "\n",
    "#### Step 2: Calculate the loss\n",
    "\n",
    "Since the true label $ y = 0 $, the formula simplifies to:\n",
    "\n",
    "$$\n",
    "L_i = - \\log(1 - \\sigma(f)) = - \\log(1 - 0.119)\n",
    "$$\n",
    "\n",
    "Now, we calculate:\n",
    "\n",
    "$$\n",
    "1 - 0.119 = 0.881\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\log(0.881) \\approx -0.127\n",
    "$$\n",
    "\n",
    "Thus, the loss for this example is:\n",
    "\n",
    "$$\n",
    "L_i = -(-0.127) = 0.127\n",
    "$$\n",
    "\n",
    "So, the loss for this example when the true label is **0** is **0.127**.\n",
    "\n",
    "---\n",
    "\n",
    "### Recap of Results:\n",
    "\n",
    "- **For true label $ y = 1 $ and logit $ f = 2.0 $:**  \n",
    "  The predicted probability is $ \\sigma(2.0) \\approx 0.881 $, and the loss is **0.127**.\n",
    "  \n",
    "- **For true label $ y = 0 $ and logit $ f = -2.0 $:**  \n",
    "  The predicted probability is $ \\sigma(-2.0) \\approx 0.119 $, and the loss is **0.127**.\n",
    "\n",
    "### Interpretation:\n",
    "- The loss for both cases is the same, **0.127**, because in both cases the model's predicted probability is relatively close to the true label (for $ y = 1 $, the probability is 0.881, and for $ y = 0 $, the probability is 0.119).\n",
    "- A smaller value of the logit results in a smaller predicted probability for class 1, and a larger value of the logit results in a larger predicted probability for class 1.\n",
    "\n",
    "This shows how the **binary cross-entropy loss** works in both positive and negative classes, ensuring that the loss penalizes wrong predictions based on how far the predicted probabilities are from the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario 1: True label = 1, Logit f = 2.0\n",
      "Sigmoid output: 0.881, Cross-entropy loss: 0.127\n",
      "\n",
      "Scenario 2: True label = 0, Logit f = -2.0\n",
      "Sigmoid output: 0.119, Cross-entropy loss: 0.127\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(f):\n",
    "    return 1 / (1 + np.exp(-f))\n",
    "\n",
    "# Binary cross-entropy loss function\n",
    "def binary_cross_entropy_loss(y_true, f):\n",
    "    prob = sigmoid(f)  # Compute sigmoid\n",
    "    loss = - (y_true * np.log(prob) + (1 - y_true) * np.log(1 - prob))\n",
    "    return prob, loss\n",
    "\n",
    "# Scenario 1: True label = 1 (Positive class)\n",
    "f_pos = 2.0\n",
    "y_true_pos = 1\n",
    "prob_pos, loss_pos = binary_cross_entropy_loss(y_true_pos, f_pos)\n",
    "\n",
    "# Scenario 2: True label = 0 (Negative class)\n",
    "f_neg = -2.0\n",
    "y_true_neg = 0\n",
    "prob_neg, loss_neg = binary_cross_entropy_loss(y_true_neg, f_neg)\n",
    "\n",
    "# Print results\n",
    "print(f\"Scenario 1: True label = {y_true_pos}, Logit f = {f_pos}\")\n",
    "print(f\"Sigmoid output: {prob_pos:.3f}, Cross-entropy loss: {loss_pos:.3f}\\n\")\n",
    "\n",
    "print(f\"Scenario 2: True label = {y_true_neg}, Logit f = {f_neg}\")\n",
    "print(f\"Sigmoid output: {prob_neg:.3f}, Cross-entropy loss: {loss_neg:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient of binary cross entropy loss\n",
    "\n",
    "For positive case, given:\n",
    "\n",
    "$$\n",
    "p = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "L = \\log p\n",
    "$$\n",
    "\n",
    "What is the gradient of the loss L with regard to x (usually called as logit):\n",
    "\n",
    "$$\n",
    "\\frac{dL}{dx}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Compute $ \\frac{dp}{dx} $**\n",
    "\n",
    "We rewrite $ p $:\n",
    "\n",
    "$$\n",
    "p = (1 + e^{-x})^{-1}\n",
    "$$\n",
    "\n",
    "Differentiate both sides using the chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{dp}{dx} = - (1 + e^{-x})^{-2} \\cdot \\frac{d}{dx} (1 + e^{-x})\n",
    "$$\n",
    "\n",
    "Since:\n",
    "\n",
    "$$\n",
    "\\frac{d}{dx} (1 + e^{-x}) = -e^{-x}\n",
    "$$\n",
    "\n",
    "we get:\n",
    "\n",
    "$$\n",
    "\\frac{dp}{dx} = - (1 + e^{-x})^{-2} \\cdot (-e^{-x})\n",
    "$$\n",
    "\n",
    "Simplifying:\n",
    "\n",
    "$$\n",
    "\\frac{dp}{dx} = \\frac{e^{-x}}{(1 + e^{-x})^2}\n",
    "$$\n",
    "\n",
    "Rewriting in terms of $ p $:\n",
    "\n",
    "$$\n",
    "p = \\frac{1}{1 + e^{-x}}, \\quad 1 - p = \\frac{e^{-x}}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "\\frac{dp}{dx} = p(1 - p)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Compute $ \\frac{dL}{dx} $**\n",
    "\n",
    "Since $ L = \\log p $, we differentiate:\n",
    "\n",
    "$$\n",
    "\\frac{dL}{dx} = \\frac{1}{p} \\cdot \\frac{dp}{dx}\n",
    "$$\n",
    "\n",
    "Substituting $ \\frac{dp}{dx} = p(1 - p) $:\n",
    "\n",
    "$$\n",
    "\\frac{dL}{dx} = \\frac{p(1 - p)}{p}\n",
    "$$\n",
    "\n",
    "Cancel $ p $:\n",
    "\n",
    "$$\n",
    "\\frac{dL}{dx} = 1 - p\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Final Answer**\n",
    "$$\n",
    "\\frac{dL}{dx} = 1 - p\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical derivative: 0.26894043831382497\n",
      "Analytical derivative: 0.2689414213699951\n",
      "Difference: 9.830561701340557e-07\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def loss(x):\n",
    "    p = sigmoid(x)\n",
    "    return np.log(p)\n",
    "\n",
    "def analytical_derivative(x):\n",
    "    p = sigmoid(x)\n",
    "    return 1 - p\n",
    "\n",
    "# Define x and a small step h\n",
    "x = 1.0  # Example value\n",
    "h = 1e-5\n",
    "\n",
    "# Compute L at x and x + h\n",
    "L_x = loss(x)\n",
    "L_x_h = loss(x + h)\n",
    "\n",
    "# Compute numerical derivative\n",
    "numerical_derivative = (L_x_h - L_x) / h\n",
    "\n",
    "# Compute analytical derivative\n",
    "analytical_result = analytical_derivative(x)\n",
    "\n",
    "# Print results\n",
    "print(f\"Numerical derivative: {numerical_derivative}\")\n",
    "print(f\"Analytical derivative: {analytical_result}\")\n",
    "\n",
    "# Check if they are approximately equal\n",
    "print(f\"Difference: {abs(numerical_derivative - analytical_result)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For negative case, we are given:  \n",
    "\n",
    "$$\n",
    "p = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "L = \\log(1 - p)\n",
    "$$\n",
    "\n",
    "We need to compute:\n",
    "\n",
    "$$\n",
    "\\frac{dL}{dx}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Compute $ \\frac{dp}{dx} $**  \n",
    "\n",
    "From previous derivations, we know:\n",
    "\n",
    "$$\n",
    "\\frac{dp}{dx} = p(1 - p)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Compute $ \\frac{dL}{dx} $**  \n",
    "\n",
    "Since $ L = \\log(1 - p) $, we differentiate using the chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{dL}{dx} = \\frac{1}{1 - p} \\cdot \\frac{d(1 - p)}{dx}\n",
    "$$\n",
    "\n",
    "Since:\n",
    "\n",
    "$$\n",
    "\\frac{d(1 - p)}{dx} = -\\frac{dp}{dx} = -p(1 - p)\n",
    "$$\n",
    "\n",
    "we substitute:\n",
    "\n",
    "$$\n",
    "\\frac{dL}{dx} = \\frac{1}{1 - p} \\cdot (-p(1 - p))\n",
    "$$\n",
    "\n",
    "Cancel $ 1 - p $:\n",
    "\n",
    "$$\n",
    "\\frac{dL}{dx} = -p\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Final Answer**\n",
    "$$\n",
    "\\frac{dL}{dx} = -p\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical derivative: -0.7310595617093795\n",
      "Analytical derivative: -0.7310585786300049\n",
      "Difference: 9.830793745724264e-07\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def loss(x):\n",
    "    p = sigmoid(x)\n",
    "    return np.log(1 - p)\n",
    "\n",
    "def analytical_derivative(x):\n",
    "    p = sigmoid(x)\n",
    "    return -p\n",
    "\n",
    "# Define x and a small step h\n",
    "x = 1.0  # Example value\n",
    "h = 1e-5\n",
    "\n",
    "# Compute L at x and x + h\n",
    "L_x = loss(x)\n",
    "L_x_h = loss(x + h)\n",
    "\n",
    "# Compute numerical derivative\n",
    "numerical_derivative = (L_x_h - L_x) / h\n",
    "\n",
    "# Compute analytical derivative\n",
    "analytical_result = analytical_derivative(x)\n",
    "\n",
    "# Print results\n",
    "print(f\"Numerical derivative: {numerical_derivative}\")\n",
    "print(f\"Analytical derivative: {analytical_result}\")\n",
    "\n",
    "# Check if they are approximately equal\n",
    "print(f\"Difference: {abs(numerical_derivative - analytical_result)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-class Cross-Entropy Loss Explained with Example\n",
    "\n",
    "Here's an example with some sample data showing how cross-entropy loss is calculated.\n",
    "\n",
    "### Example:\n",
    "Suppose we have a classification problem with 3 classes, and for a particular sample, the predicted scores (logits) and true label are as follows:\n",
    "\n",
    "- **Predicted logits (scores)**: $ f = [2.0, 1.0, 0.1] $\n",
    "- **True label**: $ y_i = 0 $ (this means the true class is the first class)\n",
    "\n",
    "### Step-by-step Calculation:\n",
    "\n",
    "1. **Apply the softmax function** to the logits to get the predicted probabilities.\n",
    "\n",
    "The softmax function is given by:\n",
    "$ \\text{softmax}(f_j) = \\frac{e^{f_j}}{\\sum_{k} e^{f_k}}$\n",
    "\n",
    "For each class $ j $, we compute $ e^{f_j} $, then normalize the result by dividing by the sum of all the exponentiated values.\n",
    "\n",
    "- Exponentiating the logits:\n",
    "  - $ e^{f_0} = e^{2.0} = 7.389 $\n",
    "  - $ e^{f_1} = e^{1.0} = 2.718 $\n",
    "  - $ e^{f_2} = e^{0.1} = 1.105 $\n",
    "\n",
    "- Sum of the exponentiated logits:\n",
    "  - $ \\text{Sum} = 7.389 + 2.718 + 1.105 = 11.212 $\n",
    "\n",
    "- Predicted probabilities (softmax values):\n",
    "  - $ p_0 = \\frac{7.389}{11.212} = 0.659 $\n",
    "  - $ p_1 = \\frac{2.718}{11.212} = 0.242 $\n",
    "  - $ p_2 = \\frac{1.105}{11.212} = 0.098 $\n",
    "\n",
    "2. **Calculate the cross-entropy loss**.\n",
    "\n",
    "The cross-entropy loss for this sample is computed as:\n",
    "\n",
    "$\n",
    "L_i = - \\log(p_{y_i})\n",
    "$\n",
    "\n",
    "Since the true label is class 0 ($ y_i = 0 $), we use the predicted probability for class 0:\n",
    "\n",
    "$\n",
    "L_i = - \\log(0.659) = -(-0.417) = 0.417\n",
    "$\n",
    "\n",
    "### Final Result:\n",
    "\n",
    "So, the cross-entropy loss for this sample is **0.417**. \n",
    "\n",
    "This value indicates the penalty for the incorrect predictions, with a lower value meaning better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Given logits\n",
    "logits = np.array([2.0, 1.0, 0.1])\n",
    "\n",
    "# Step 1: Compute softmax\n",
    "exp_logits = np.exp(logits)  # Exponentiate each logit\n",
    "softmax_probs = exp_logits / np.sum(exp_logits)  # Normalize\n",
    "\n",
    "# Step 2: Compute cross-entropy loss\n",
    "true_label = 0  # Given that the true class is 0\n",
    "loss = -np.log(softmax_probs[true_label])\n",
    "\n",
    "# Print results\n",
    "print(f\"Exponentiated logits: {exp_logits}\")\n",
    "print(f\"Softmax probabilities: {softmax_probs}\")\n",
    "print(f\"Cross-entropy loss: {loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
