{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b851f385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import scaled_dot_product_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2667fe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch.nn.functional import scaled_dot_product_attention\n",
    "\n",
    "def create_test_data_for_extend():\n",
    "    \"\"\"\n",
    "    Create sample data to test _run_sdpa_forward_extend function.\n",
    "    \n",
    "    Scenario: 3 sequences being extended with different configurations\n",
    "    \"\"\"\n",
    "    \n",
    "    # Model configuration\n",
    "    num_heads = 8\n",
    "    head_size = 64\n",
    "    max_total_tokens = 1000\n",
    "    max_num_reqs = 100\n",
    "    max_context_len = 512\n",
    "    \n",
    "    # Batch configuration\n",
    "    num_seqs = 3\n",
    "    seq_lens = torch.tensor([10, 15, 8])                # Total tokens after extension\n",
    "    extend_prefix_lens = torch.tensor([7, 12, 5])       # Already cached tokens\n",
    "    extend_seq_lens = torch.tensor([3, 3, 3])           # New tokens being added\n",
    "    req_pool_indices = torch.tensor([42, 17, 89])       # Memory pool locations\n",
    "    \n",
    "    # Calculate total new tokens\n",
    "    num_tokens = extend_seq_lens.sum().item()  # 3 + 3 + 3 = 9\n",
    "    \n",
    "    print(f\"=== TEST DATA CONFIGURATION ===\")\n",
    "    print(f\"num_seqs: {num_seqs}\")\n",
    "    print(f\"seq_lens: {seq_lens.tolist()}\")\n",
    "    print(f\"extend_prefix_lens: {extend_prefix_lens.tolist()}\")\n",
    "    print(f\"extend_seq_lens: {extend_seq_lens.tolist()}\")\n",
    "    print(f\"total num_tokens: {num_tokens}\")\n",
    "    print(f\"req_pool_indices: {req_pool_indices.tolist()}\")\n",
    "    print()\n",
    "    \n",
    "    # Create query tensor [num_tokens, num_heads, head_size]\n",
    "    query = torch.randn(num_tokens, num_heads, head_size)\n",
    "    \n",
    "    # Mark query tokens with identifiable values for debugging\n",
    "    for i in range(num_tokens):\n",
    "        query[i, :, 0] = i + 100  # First dimension has token ID (100, 101, 102, ...)\n",
    "    \n",
    "    print(f\"Query tensor shape: {query.shape}\")\n",
    "    print(f\"Query token markers (first head, first dim): {query[:, 0, 0].tolist()}\")\n",
    "    print()\n",
    "    \n",
    "    # Create output tensor (same shape as query)\n",
    "    output = torch.zeros_like(query)\n",
    "    \n",
    "    # Create global KV cache [max_total_tokens, num_heads, head_size]\n",
    "    k_cache = torch.randn(max_total_tokens, num_heads, head_size)\n",
    "    v_cache = torch.randn(max_total_tokens, num_heads, head_size)\n",
    "    \n",
    "    # Mark cache entries with identifiable values\n",
    "    for i in range(max_total_tokens):\n",
    "        k_cache[i, :, 0] = i + 1000  # Key cache markers (1000, 1001, 1002, ...)\n",
    "        v_cache[i, :, 0] = i + 2000  # Value cache markers (2000, 2001, 2002, ...)\n",
    "    \n",
    "    # Create req_to_token mapping [max_num_reqs, max_context_len]\n",
    "    req_to_token = torch.zeros(max_num_reqs, max_context_len, dtype=torch.long)\n",
    "    \n",
    "    # Set up token mappings for our test sequences\n",
    "    # Sequence 0: pool index 42, uses tokens 100-109 in cache\n",
    "    req_to_token[42, :10] = torch.arange(100, 110)\n",
    "    \n",
    "    # Sequence 1: pool index 17, uses tokens 200-214 in cache  \n",
    "    req_to_token[17, :15] = torch.arange(200, 215)\n",
    "    \n",
    "    # Sequence 2: pool index 89, uses tokens 300-307 in cache\n",
    "    req_to_token[89, :8] = torch.arange(300, 308)\n",
    "    \n",
    "    print(f\"=== TOKEN MAPPINGS ===\")\n",
    "    print(f\"Seq 0 (pool {req_pool_indices[0]}): cache tokens {req_to_token[42, :10].tolist()}\")\n",
    "    print(f\"Seq 1 (pool {req_pool_indices[1]}): cache tokens {req_to_token[17, :15].tolist()}\")\n",
    "    print(f\"Seq 2 (pool {req_pool_indices[2]}): cache tokens {req_to_token[89, :8].tolist()}\")\n",
    "    print()\n",
    "    \n",
    "    # Set scaling factor\n",
    "    scaling = 1.0 / math.sqrt(head_size)\n",
    "    \n",
    "    return {\n",
    "        'query': query,\n",
    "        'output': output,\n",
    "        'k_cache': k_cache,\n",
    "        'v_cache': v_cache,\n",
    "        'req_to_token': req_to_token,\n",
    "        'req_pool_indices': req_pool_indices,\n",
    "        'seq_lens': seq_lens,\n",
    "        'extend_prefix_lens': extend_prefix_lens,\n",
    "        'extend_seq_lens': extend_seq_lens,\n",
    "        'scaling': scaling,\n",
    "        'enable_gqa': False,\n",
    "        'causal': True\n",
    "    }\n",
    "\n",
    "def test_run_sdpa_forward_extend():\n",
    "    \"\"\"Test the _run_sdpa_forward_extend function with sample data.\"\"\"\n",
    "    \n",
    "    # Get test data\n",
    "    test_data = create_test_data_for_extend()\n",
    "    \n",
    "    # Extract parameters\n",
    "    query = test_data['query']\n",
    "    output = test_data['output']\n",
    "    k_cache = test_data['k_cache']\n",
    "    v_cache = test_data['v_cache']\n",
    "    req_to_token = test_data['req_to_token']\n",
    "    req_pool_indices = test_data['req_pool_indices']\n",
    "    seq_lens = test_data['seq_lens']\n",
    "    extend_prefix_lens = test_data['extend_prefix_lens']\n",
    "    extend_seq_lens = test_data['extend_seq_lens']\n",
    "    scaling = test_data['scaling']\n",
    "    enable_gqa = test_data['enable_gqa']\n",
    "    causal = test_data['causal']\n",
    "    \n",
    "    print(\"=== BEFORE PROCESSING ===\")\n",
    "    print(f\"Output tensor (should be zeros): {output[:3, 0, 0].tolist()}\")\n",
    "    print()\n",
    "    \n",
    "    # Simulate the _run_sdpa_forward_extend function\n",
    "    # [num_tokens, num_heads, head_size] -> [num_heads, num_tokens, head_size]\n",
    "    query_reshaped = query.movedim(0, query.dim() - 2)\n",
    "    \n",
    "    start_q, start_kv = 0, 0\n",
    "    \n",
    "    for seq_idx in range(seq_lens.shape[0]):\n",
    "        print(f\"=== PROCESSING SEQUENCE {seq_idx} ===\")\n",
    "        \n",
    "        extend_seq_len_q = extend_seq_lens[seq_idx].item()\n",
    "        prefill_seq_len_q = extend_prefix_lens[seq_idx].item()\n",
    "        seq_len_kv = seq_lens[seq_idx].item()\n",
    "        end_q = start_q + extend_seq_len_q\n",
    "        end_kv = start_kv + seq_len_kv\n",
    "        \n",
    "        print(f\"extend_seq_len_q: {extend_seq_len_q}\")\n",
    "        print(f\"prefill_seq_len_q: {prefill_seq_len_q}\")\n",
    "        print(f\"seq_len_kv: {seq_len_kv}\")\n",
    "        print(f\"Query range: [{start_q}:{end_q}]\")\n",
    "        \n",
    "        # Extract query for this sequence\n",
    "        per_req_query = query_reshaped[:, start_q:end_q, :]\n",
    "        print(f\"per_req_query shape: {per_req_query.shape}\")\n",
    "        print(f\"per_req_query markers: {per_req_query[0, :, 0].tolist()}\")\n",
    "        \n",
    "        # Create redundant query tensor\n",
    "        per_req_query_redundant = torch.zeros(\n",
    "            (per_req_query.shape[0], seq_len_kv, per_req_query.shape[2]),\n",
    "            dtype=per_req_query.dtype,\n",
    "            device=per_req_query.device,\n",
    "        )\n",
    "        \n",
    "        # Place new queries at correct positions  \n",
    "        per_req_query_redundant[:, prefill_seq_len_q:, :] = per_req_query\n",
    "        print(f\"per_req_query_redundant shape: {per_req_query_redundant.shape}\")\n",
    "        print(f\"Redundant query markers: {per_req_query_redundant[0, :, 0].tolist()}\")\n",
    "        \n",
    "        # Get key and value from cache\n",
    "        req_pool_idx = req_pool_indices[seq_idx].item()\n",
    "        per_req_tokens = req_to_token[req_pool_idx, :seq_len_kv]\n",
    "        print(f\"req_pool_idx: {req_pool_idx}\")\n",
    "        print(f\"per_req_tokens: {per_req_tokens.tolist()}\")\n",
    "        \n",
    "        per_req_key = k_cache[per_req_tokens].movedim(0, query.dim() - 2)\n",
    "        per_req_value = v_cache[per_req_tokens].movedim(0, query.dim() - 2)\n",
    "        print(f\"per_req_key shape: {per_req_key.shape}\")\n",
    "        print(f\"Key markers: {per_req_key[0, :, 0].tolist()}\")\n",
    "        print(f\"Value markers: {per_req_value[0, :, 0].tolist()}\")\n",
    "        \n",
    "        # Run attention\n",
    "        per_req_out_redundant = (\n",
    "            scaled_dot_product_attention(\n",
    "                per_req_query_redundant.unsqueeze(0),\n",
    "                per_req_key.unsqueeze(0),\n",
    "                per_req_value.unsqueeze(0),\n",
    "                scale=scaling,\n",
    "                is_causal=causal,\n",
    "            )\n",
    "            .squeeze(0)\n",
    "            .movedim(query.dim() - 2, 0)\n",
    "        )\n",
    "        \n",
    "        print(f\"per_req_out_redundant shape: {per_req_out_redundant.shape}\")\n",
    "        print(f\"Output markers before extraction: {per_req_out_redundant[:, 0, 0].tolist()}\")\n",
    "        \n",
    "        # Extract relevant outputs\n",
    "        relevant_output = per_req_out_redundant[prefill_seq_len_q:, :, :]\n",
    "        output[start_q:end_q, :, :] = relevant_output\n",
    "        \n",
    "        print(f\"Extracted output shape: {relevant_output.shape}\")\n",
    "        print(f\"Placed in output[{start_q}:{end_q}]\")\n",
    "        print(f\"Output after placement: {output[start_q:end_q, 0, 0].tolist()}\")\n",
    "        print()\n",
    "        \n",
    "        start_q, start_kv = end_q, end_kv\n",
    "    \n",
    "    print(\"=== FINAL RESULTS ===\")\n",
    "    print(f\"Final output shape: {output.shape}\")\n",
    "    print(f\"Output sample (first head, first dim): {output[:, 0, 0].tolist()}\")\n",
    "    print()\n",
    "    \n",
    "    # Verify output is non-zero (attention worked)\n",
    "    output_norm = torch.norm(output)\n",
    "    print(f\"Output tensor norm: {output_norm:.4f}\")\n",
    "    \n",
    "    if output_norm > 0:\n",
    "        print(\"✅ Test PASSED - Output is non-zero, attention computation worked\")\n",
    "    else:\n",
    "        print(\"❌ Test FAILED - Output is zero, something went wrong\")\n",
    "    \n",
    "    return output\n",
    "\n",
    "def visualize_attention_pattern_correct():\n",
    "    \"\"\"\n",
    "    Properly visualize attention patterns without tensor dimension issues.\n",
    "    \n",
    "    The key insight: we need to avoid zero queries that cause NaN when combined with causal masking.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== CORRECTED ATTENTION PATTERN VISUALIZATION ===\")\n",
    "    \n",
    "    # Configuration\n",
    "    extend_prefix_len = 2  # 2 cached tokens\n",
    "    extend_seq_len = 2     # 2 new tokens  \n",
    "    seq_len_kv = 4         # Total tokens in sequence\n",
    "    \n",
    "    # Create meaningful queries (non-zero for positions that will be used)\n",
    "    # Shape: [seq_len, head_dim] where we only care about the new token positions\n",
    "    query_redundant = torch.tensor([\n",
    "        [0.0],  # Position 0: cached (will be masked anyway)\n",
    "        [0.0],  # Position 1: cached (will be masked anyway)  \n",
    "        [1.0],  # Position 2: new token with query\n",
    "        [2.0],  # Position 3: new token with query\n",
    "    ])\n",
    "    \n",
    "    # Keys and values\n",
    "    k_cache = torch.tensor([\n",
    "        [1.0],  # Key for position 0\n",
    "        [1.0],  # Key for position 1\n",
    "        [1.0],  # Key for position 2  \n",
    "        [1.0],  # Key for position 3\n",
    "    ])\n",
    "    \n",
    "    v_cache = torch.tensor([\n",
    "        [10.0],  # Value for position 0\n",
    "        [20.0],  # Value for position 1\n",
    "        [30.0],  # Value for position 2\n",
    "        [40.0],  # Value for position 3\n",
    "    ])\n",
    "    \n",
    "    print(f\"Query (redundant): {query_redundant.squeeze().tolist()}\")\n",
    "    print(f\"Key cache: {k_cache.squeeze().tolist()}\")  \n",
    "    print(f\"Value cache: {v_cache.squeeze().tolist()}\")\n",
    "    print(f\"Extend prefix length: {extend_prefix_len}\")\n",
    "    print(f\"New token positions: {extend_prefix_len} onwards\")\n",
    "    print()\n",
    "    \n",
    "    # Compute raw attention scores: Q @ K^T\n",
    "    scores = torch.matmul(query_redundant, k_cache.transpose(-2, -1))  # [4, 4]\n",
    "    print(f\"Raw scores shape: {scores.shape}\")\n",
    "    print(\"Raw scores (Q @ K^T):\")\n",
    "    for i, row in enumerate(scores.tolist()):\n",
    "        print(f\"  Position {i}: {row}\")\n",
    "    print()\n",
    "    \n",
    "    # Create causal mask: upper triangular with -inf\n",
    "    causal_mask = torch.triu(torch.ones(4, 4), diagonal=1) * float('-inf')\n",
    "    print(\"Causal mask:\")\n",
    "    for i, row in enumerate(causal_mask.tolist()):\n",
    "        print(f\"  Position {i}: {[f'{x:.0f}' if x != float('-inf') else '-∞' for x in row]}\")\n",
    "    print()\n",
    "    \n",
    "    # Apply causal mask\n",
    "    scores_masked = scores + causal_mask\n",
    "    print(\"Scores after causal masking:\")\n",
    "    for i, row in enumerate(scores_masked.tolist()):\n",
    "        formatted_row = []\n",
    "        for x in row:\n",
    "            if x == float('-inf'):\n",
    "                formatted_row.append('-∞')\n",
    "            elif math.isnan(x):\n",
    "                formatted_row.append('NaN')  \n",
    "            else:\n",
    "                formatted_row.append(f'{x:.1f}')\n",
    "        print(f\"  Position {i}: {formatted_row}\")\n",
    "    print()\n",
    "    \n",
    "    # Apply softmax to get attention weights\n",
    "    attn_weights = torch.softmax(scores_masked, dim=-1)\n",
    "    print(\"Attention weights (after softmax):\")\n",
    "    for i, row in enumerate(attn_weights.tolist()):\n",
    "        formatted_row = []\n",
    "        for x in row:\n",
    "            if math.isnan(x):\n",
    "                formatted_row.append('NaN')\n",
    "            else:\n",
    "                formatted_row.append(f'{x:.3f}')\n",
    "        print(f\"  Position {i}: {formatted_row}\")\n",
    "    print()\n",
    "    \n",
    "    # Compute final output: attention_weights @ V\n",
    "    output = torch.matmul(attn_weights, v_cache)  # [4, 1]\n",
    "    print(f\"Final output shape: {output.shape}\")\n",
    "    print(\"Final output (attention @ V):\")\n",
    "    for i, val in enumerate(output.squeeze().tolist()):\n",
    "        if math.isnan(val):\n",
    "            print(f\"  Position {i}: NaN\")\n",
    "        else:\n",
    "            print(f\"  Position {i}: {val:.2f}\")\n",
    "    print()\n",
    "    \n",
    "    # Extract only the outputs for new tokens (what the extend function would return)\n",
    "    new_token_outputs = output[extend_prefix_len:]\n",
    "    print(f\"New token outputs (positions {extend_prefix_len}+): {new_token_outputs.squeeze().tolist()}\")\n",
    "    \n",
    "    print(\"\\n=== EXPLANATION ===\")\n",
    "    print(\"1. Positions 0,1: Cached tokens (prefix)\")\n",
    "    print(\"2. Positions 2,3: New tokens being processed\")\n",
    "    print(\"3. Causal mask ensures:\")\n",
    "    print(\"   - Position 0: Only attends to itself\")\n",
    "    print(\"   - Position 1: Attends to positions 0,1\") \n",
    "    print(\"   - Position 2: Attends to positions 0,1,2\")\n",
    "    print(\"   - Position 3: Attends to positions 0,1,2,3\")\n",
    "    print(\"4. Only outputs for positions 2,3 are used (new tokens)\")\n",
    "    \n",
    "    # Show that this matches the extend function behavior\n",
    "    print(\"\\n=== EXTEND FUNCTION SIMULATION ===\")\n",
    "    print(\"This is exactly what _run_sdpa_forward_extend does:\")\n",
    "    print(\"1. Creates redundant query with new tokens at correct positions\")\n",
    "    print(\"2. Uses full KV cache for the sequence\") \n",
    "    print(\"3. Applies causal attention\")\n",
    "    print(\"4. Extracts outputs only for new token positions\")\n",
    "\n",
    "def simple_working_example():\n",
    "    \"\"\"A simple example that definitely works without any issues.\"\"\"\n",
    "    \n",
    "    print(\"=== SIMPLE WORKING EXAMPLE ===\")\n",
    "    \n",
    "    # Use PyTorch's SDPA directly (like the real function does)\n",
    "    batch_size = 1\n",
    "    num_heads = 2\n",
    "    seq_len = 4  \n",
    "    head_dim = 8\n",
    "    \n",
    "    # Create test tensors with proper dimensions\n",
    "    query = torch.randn(batch_size, num_heads, seq_len, head_dim)\n",
    "    key = torch.randn(batch_size, num_heads, seq_len, head_dim)  \n",
    "    value = torch.randn(batch_size, num_heads, seq_len, head_dim)\n",
    "    \n",
    "    print(f\"Query shape: {query.shape}\")\n",
    "    print(f\"Key shape: {key.shape}\")\n",
    "    print(f\"Value shape: {value.shape}\")\n",
    "    \n",
    "    # Run attention with causal masking\n",
    "    output = scaled_dot_product_attention(\n",
    "        query, key, value,\n",
    "        is_causal=True,\n",
    "        scale=1.0 / math.sqrt(head_dim)\n",
    "    )\n",
    "    \n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Output norm: {torch.norm(output):.4f}\")\n",
    "    \n",
    "    # Simulate extracting new tokens (last 2 positions)\n",
    "    extend_prefix_len = 2\n",
    "    new_token_outputs = output[:, :, extend_prefix_len:, :]\n",
    "    print(f\"New token outputs shape: {new_token_outputs.shape}\")\n",
    "    print(f\"New token outputs norm: {torch.norm(new_token_outputs):.4f}\")\n",
    "    \n",
    "    print(\"✅ Simple example completed successfully!\")\n",
    "    print(\"This demonstrates that SDPA works correctly with proper tensor shapes.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the simple working example first\n",
    "    simple_working_example()\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # Run the corrected visualization\n",
    "    visualize_attention_pattern_correct()\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # Run the main test\n",
    "    test_run_sdpa_forward_extend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fac9cf7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TEST DATA CONFIGURATION ===\n",
      "num_seqs: 3\n",
      "seq_lens: [10, 15, 8]\n",
      "extend_prefix_lens: [7, 12, 5]\n",
      "extend_seq_lens: [3, 3, 3]\n",
      "total num_tokens: 9\n",
      "req_pool_indices: [42, 17, 89]\n",
      "\n",
      "Query tensor shape: torch.Size([9, 8, 64])\n",
      "Query token markers (first head, first dim): [100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0]\n",
      "\n",
      "=== TOKEN MAPPINGS ===\n",
      "Seq 0 (pool 42): cache tokens [100, 101, 102, 103, 104, 105, 106, 107, 108, 109]\n",
      "Seq 1 (pool 17): cache tokens [200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214]\n",
      "Seq 2 (pool 89): cache tokens [300, 301, 302, 303, 304, 305, 306, 307]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testdata = create_test_data_for_extend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e983171e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0000e+02, -1.6576e+00, -2.0029e-02, -4.2802e-01,  1.3332e+00,\n",
       "         9.9924e-01, -8.8784e-01, -4.4433e-01, -5.4830e-01,  5.9202e-01,\n",
       "         8.4495e-01, -8.9862e-01, -2.5020e-02, -7.3002e-01, -2.1434e+00,\n",
       "         6.3140e-01,  5.9199e-01,  2.7241e+00,  1.6477e+00,  8.3054e-01,\n",
       "         1.6440e+00, -1.1733e+00,  6.0557e-01,  1.8598e-01, -1.8381e+00,\n",
       "         5.8024e-01,  4.4517e-01, -2.8377e-01, -1.4314e+00,  1.1170e-02,\n",
       "         1.3062e+00, -1.5280e+00, -7.7959e-01, -9.0410e-01,  1.5533e+00,\n",
       "         6.5476e-01, -1.0017e+00, -3.6100e-01,  1.6596e+00, -6.9675e-01,\n",
       "        -6.5668e-02,  8.1829e-01, -1.2362e+00, -8.3845e-01,  3.2537e-01,\n",
       "        -1.8887e-01,  1.4897e-01, -9.9384e-01,  7.2069e-01,  1.8491e+00,\n",
       "        -9.2455e-01,  6.9416e-01, -1.2503e+00, -3.9238e-01,  1.7303e+00,\n",
       "        -2.0750e+00, -1.0172e+00, -4.9965e-01, -7.6738e-01,  1.5230e+00,\n",
       "         6.1120e-01,  4.1065e-01, -7.2143e-01,  1.1637e+00])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = testdata['query']\n",
    "q[0, 1, :]  # Access the first element in the first head and first dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "42ec042f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_q: 0, end_q: 3\n",
      "per_req_query shape: torch.Size([8, 3, 64])\n",
      "per_req_query_redudant shape: torch.Size([8, 10, 64])\n",
      "per_req_query_redudant after assignment shape: torch.Size([8, 10, 64])\n",
      "start_q: 3, end_q: 6\n",
      "per_req_query shape: torch.Size([8, 3, 64])\n",
      "per_req_query_redudant shape: torch.Size([8, 15, 64])\n",
      "per_req_query_redudant after assignment shape: torch.Size([8, 15, 64])\n",
      "start_q: 6, end_q: 9\n",
      "per_req_query shape: torch.Size([8, 3, 64])\n",
      "per_req_query_redudant shape: torch.Size([8, 8, 64])\n",
      "per_req_query_redudant after assignment shape: torch.Size([8, 8, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.1070e+03, -1.7710e+00, -1.1017e+00,  ..., -4.2823e-01,\n",
       "           1.4145e-01, -2.0578e+00],\n",
       "         [ 2.1070e+03,  1.5543e-01,  1.3160e-01,  ...,  4.8716e-01,\n",
       "           9.3702e-01,  1.3232e-01],\n",
       "         [ 2.1070e+03,  2.2299e-01,  1.4251e-02,  ..., -3.9241e-01,\n",
       "          -9.8241e-01,  5.5761e-02],\n",
       "         ...,\n",
       "         [ 2.1070e+03,  8.1138e-01, -1.0179e+00,  ...,  9.7894e-01,\n",
       "          -7.9975e-01, -8.9252e-01],\n",
       "         [ 2.1070e+03,  1.9310e+00, -6.5271e-01,  ...,  5.2144e-01,\n",
       "          -3.0845e-01, -3.9937e-01],\n",
       "         [ 2.1070e+03, -8.9899e-01, -2.8282e-02,  ...,  2.2493e+00,\n",
       "           4.6407e-01, -1.1943e+00]],\n",
       "\n",
       "        [[ 2.1080e+03,  1.1631e+00,  9.3131e-01,  ...,  9.6877e-01,\n",
       "          -5.1081e-01, -1.7506e+00],\n",
       "         [ 2.1080e+03, -5.8837e-01, -5.5931e-01,  ..., -4.2122e-01,\n",
       "          -1.2030e-01,  8.3382e-01],\n",
       "         [ 2.1080e+03,  3.8540e-01, -5.2342e-02,  ...,  2.2130e+00,\n",
       "          -5.2050e-01, -5.6921e-01],\n",
       "         ...,\n",
       "         [ 2.1080e+03,  1.9078e+00,  8.5422e-02,  ..., -8.8960e-01,\n",
       "           5.3803e-01, -1.3845e+00],\n",
       "         [ 2.1080e+03,  3.5212e-02,  3.4514e-01,  ..., -3.8371e-01,\n",
       "           1.9395e-01,  6.0892e-01],\n",
       "         [ 2.1080e+03, -1.7624e+00,  3.4297e-01,  ...,  4.4613e-01,\n",
       "           7.0775e-01,  7.7757e-01]],\n",
       "\n",
       "        [[ 2.1090e+03, -5.1478e-01,  7.1301e-01,  ...,  1.0259e+00,\n",
       "          -1.6445e+00, -4.3856e-01],\n",
       "         [ 2.1090e+03,  1.1627e+00,  1.4340e-02,  ..., -3.3341e-01,\n",
       "          -2.4828e-01, -2.2452e-01],\n",
       "         [ 2.1090e+03, -7.2029e-02, -6.2146e-01,  ...,  1.1885e+00,\n",
       "           7.6897e-01,  2.7541e-01],\n",
       "         ...,\n",
       "         [ 2.1090e+03,  1.7910e+00, -1.0506e+00,  ...,  1.6791e+00,\n",
       "          -6.7729e-03, -5.7719e-01],\n",
       "         [ 2.1090e+03, -1.9308e+00, -5.8088e-01,  ..., -3.8565e-01,\n",
       "           1.5949e-01, -9.4311e-01],\n",
       "         [ 2.1090e+03,  6.0095e-01, -1.4037e-04,  ...,  1.1243e+00,\n",
       "          -1.6204e-02, -1.0366e+00]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 2.3050e+03, -7.6403e-02,  6.1560e-01,  ...,  4.2751e-02,\n",
       "           1.1008e+00,  9.6517e-01],\n",
       "         [ 2.3050e+03,  4.1542e-01, -4.5352e-01,  ...,  2.0406e+00,\n",
       "           1.2705e+00, -2.6073e-01],\n",
       "         [ 2.3050e+03, -7.6536e-01, -1.2600e+00,  ...,  3.1358e-01,\n",
       "          -1.2703e+00,  5.4326e-01],\n",
       "         ...,\n",
       "         [ 2.3050e+03,  2.2290e-01,  1.5344e-01,  ..., -1.4058e-01,\n",
       "           8.3305e-02,  7.9759e-01],\n",
       "         [ 2.3050e+03, -1.0634e+00,  1.9440e+00,  ..., -2.3108e-02,\n",
       "          -3.2933e-01,  6.5075e-01],\n",
       "         [ 2.3050e+03,  4.4842e-01,  8.9490e-02,  ...,  5.6924e-01,\n",
       "          -7.8167e-01,  1.9660e-01]],\n",
       "\n",
       "        [[ 2.3060e+03, -1.6968e-01, -1.0103e+00,  ..., -9.6821e-02,\n",
       "          -9.4408e-01, -1.6725e+00],\n",
       "         [ 2.3060e+03, -2.1120e+00,  1.3464e+00,  ...,  3.4182e-01,\n",
       "          -1.4757e+00,  1.2673e+00],\n",
       "         [ 2.3060e+03, -1.2926e+00,  1.5036e-01,  ..., -1.2132e+00,\n",
       "          -1.9224e+00,  1.2531e+00],\n",
       "         ...,\n",
       "         [ 2.3060e+03,  1.4017e-01, -1.1969e+00,  ..., -1.4019e+00,\n",
       "          -1.9966e+00, -5.0398e-01],\n",
       "         [ 2.3060e+03,  6.6054e-01,  3.4992e-01,  ...,  2.0130e+00,\n",
       "          -1.4138e+00,  5.6182e-01],\n",
       "         [ 2.3060e+03, -6.6071e-01, -1.2509e-01,  ..., -2.0713e+00,\n",
       "          -7.3790e-01,  3.9548e-01]],\n",
       "\n",
       "        [[ 2.3070e+03,  3.7723e-03, -3.2376e-01,  ..., -8.6301e-01,\n",
       "           1.1171e+00,  7.1906e-01],\n",
       "         [ 2.3070e+03,  2.0586e-01,  1.7998e-01,  ..., -1.5861e-01,\n",
       "          -1.5931e-02, -1.6796e+00],\n",
       "         [ 2.3070e+03,  2.9371e-01, -1.2665e+00,  ..., -7.6051e-01,\n",
       "          -1.2430e+00, -1.9786e+00],\n",
       "         ...,\n",
       "         [ 2.3070e+03, -1.0629e+00, -7.6003e-01,  ..., -2.7305e-04,\n",
       "           2.4634e+00, -1.0123e+00],\n",
       "         [ 2.3070e+03,  7.9624e-01, -8.1141e-01,  ...,  1.0282e+00,\n",
       "          -4.3110e-01,  1.2184e+00],\n",
       "         [ 2.3070e+03, -4.7438e-01, -7.2216e-01,  ...,  1.1694e-01,\n",
       "          -1.0811e+00, -2.7246e-01]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_run_sdpa_forward_extend(\n",
    "    query=testdata['query'],\n",
    "    output=testdata['output'],\n",
    "    k_cache=testdata['k_cache'],\n",
    "    v_cache=testdata['v_cache'],\n",
    "    req_to_token=testdata['req_to_token'],\n",
    "    req_pool_indices=testdata['req_pool_indices'],\n",
    "    seq_lens=testdata['seq_lens'],\n",
    "    extend_prefix_lens=testdata['extend_prefix_lens'],\n",
    "    extend_seq_lens=testdata['extend_seq_lens'],\n",
    "    scaling=testdata['scaling'],\n",
    "    enable_gqa=testdata['enable_gqa'],\n",
    "    causal=testdata['causal'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99d60977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 8, 64])\n",
      "torch.Size([8, 9, 64])\n"
     ]
    }
   ],
   "source": [
    "query = testdata['query']\n",
    "print(query.shape) \n",
    "query = query.movedim(0, query.dim() - 2)  \n",
    "print(query.shape)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52f38f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10, 15,  8])\n",
      "torch.Size([3])\n",
      "3\n",
      "tensor(10)\n"
     ]
    }
   ],
   "source": [
    "seq_lens= testdata['seq_lens']\n",
    "print(seq_lens)\n",
    "print(seq_lens.shape)\n",
    "print(seq_lens.shape[0])  # Number of sequences\n",
    "\n",
    "\n",
    "seq_len_kv = seq_lens[0]  # Example for the first sequence\n",
    "print(seq_len_kv)  # Should print the length of the first sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2bb74adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 3, 3])\n",
      "torch.Size([3])\n",
      "tensor(3)\n",
      "tensor(3)\n"
     ]
    }
   ],
   "source": [
    "extend_seq_lens = testdata['extend_seq_lens']\n",
    "print(extend_seq_lens)\n",
    "print(extend_seq_lens.shape)\n",
    "print(extend_seq_lens[0]) # Number of sequences\n",
    "\n",
    "\n",
    "extend_seq_len_q = extend_seq_lens[0]  # Example for the first sequence\n",
    "start_q = 0\n",
    "end_q = start_q + extend_seq_len_q\n",
    "print(end_q)  # Should print the length of the first sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5865c763",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 4\n",
    "head_size = 8\n",
    "max_total_tokens = 1000\n",
    "max_num_reqs = 100\n",
    "max_context_len = 512\n",
    "\n",
    "\n",
    "num_seqs = 3\n",
    "seq_lens = torch.tensor([10, 15, 8])                # Total tokens after extension\n",
    "extend_prefix_lens = torch.tensor([7, 12, 5])       # Already cached tokens\n",
    "extend_seq_lens = torch.tensor([3, 3, 3])           # New tokens being added\n",
    "req_pool_indices = torch.tensor([42, 17, 89])       # Memory pool\n",
    "\n",
    "num_tokens = extend_seq_lens.sum().item()  # 3 + 3 + 3 = 9\n",
    "\n",
    "\n",
    "print(f\"=== TEST DATA CONFIGURATION ===\")\n",
    "print(f\"num_seqs: {num_seqs}\")\n",
    "print(f\"seq_lens: {seq_lens.tolist()}\")\n",
    "print(f\"extend_prefix_lens: {extend_prefix_lens.tolist()}\")\n",
    "print(f\"extend_seq_lens: {extend_seq_lens.tolist()}\")\n",
    "print(f\"total num_tokens: {num_tokens}\")\n",
    "print(f\"req_pool_indices: {req_pool_indices.tolist()}\")\n",
    "\n",
    "query = torch.arange(num_tokens * num_heads * head_size).view(num_tokens, num_heads, head_size).float()\n",
    "print(f\"Query tensor shape: {query.shape}\")\n",
    "    # Mark query tokens with identifiable values for debugging\n",
    "for i in range(num_tokens):\n",
    "    query[i, :, 0] = i + 100  # First dimension has token ID (100, 101, 102, ...)\n",
    "    \n",
    "print(f\"Query tensor shape: {query.shape}\")\n",
    "print(f\"query: {query}\")\n",
    "print(f\"Query token markers (first head, first dim): {query[:, 0, 0].tolist()}\")\n",
    "print()\n",
    "\n",
    "# Create output tensor (same shape as query)\n",
    "output = torch.zeros_like(query)\n",
    "\n",
    "# Create global KV cache [max_total_tokens, num_heads, head_size]\n",
    "k_cache = torch.arange(max_total_tokens * num_heads * head_size).view(max_total_tokens, num_heads, head_size).float()\n",
    "v_cache = torch.arange(max_total_tokens * num_heads * head_size).view(max_total_tokens, num_heads, head_size).float()\n",
    "\n",
    "# Mark cache entries with identifiable values\n",
    "for i in range(max_total_tokens):\n",
    "    k_cache[i, :, 0] = i + 1000  # Key cache markers (1000, 1001, 1002, ...)\n",
    "    v_cache[i, :, 0] = i + 2000  # Value cache markers (2000, 2001, 2002, ...)\n",
    "\n",
    "# Create req_to_token mapping [max_num_reqs, max_context_len]\n",
    "req_to_token = torch.zeros(max_num_reqs, max_context_len, dtype=torch.long)\n",
    "\n",
    "# Set up token mappings for our test sequences\n",
    "# Sequence 0: pool index 42, uses tokens 100-109 in cache\n",
    "req_to_token[42, :10] = torch.arange(100, 110)\n",
    "    \n",
    "# Sequence 1: pool index 17, uses tokens 200-214 in cache  \n",
    "req_to_token[17, :15] = torch.arange(200, 215)\n",
    "    \n",
    "# Sequence 2: pool index 89, uses tokens 300-307 in cache\n",
    "req_to_token[89, :8] = torch.arange(300, 308)\n",
    "\n",
    "print(req_to_token[42])\n",
    "print(req_to_token[17])\n",
    "print(req_to_token[89])\n",
    "\n",
    "scaling = 1.0 / math.sqrt(head_size)\n",
    "\n",
    "enable_gqa = False\n",
    "causal = True\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "dfa90a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 4, 8])\n",
      "tensor([[[100.,   1.,   2.,   3.,   4.,   5.,   6.,   7.],\n",
      "         [100.,   9.,  10.,  11.,  12.,  13.,  14.,  15.],\n",
      "         [100.,  17.,  18.,  19.,  20.,  21.,  22.,  23.],\n",
      "         [100.,  25.,  26.,  27.,  28.,  29.,  30.,  31.]],\n",
      "\n",
      "        [[101.,  33.,  34.,  35.,  36.,  37.,  38.,  39.],\n",
      "         [101.,  41.,  42.,  43.,  44.,  45.,  46.,  47.],\n",
      "         [101.,  49.,  50.,  51.,  52.,  53.,  54.,  55.],\n",
      "         [101.,  57.,  58.,  59.,  60.,  61.,  62.,  63.]],\n",
      "\n",
      "        [[102.,  65.,  66.,  67.,  68.,  69.,  70.,  71.],\n",
      "         [102.,  73.,  74.,  75.,  76.,  77.,  78.,  79.],\n",
      "         [102.,  81.,  82.,  83.,  84.,  85.,  86.,  87.],\n",
      "         [102.,  89.,  90.,  91.,  92.,  93.,  94.,  95.]],\n",
      "\n",
      "        [[103.,  97.,  98.,  99., 100., 101., 102., 103.],\n",
      "         [103., 105., 106., 107., 108., 109., 110., 111.],\n",
      "         [103., 113., 114., 115., 116., 117., 118., 119.],\n",
      "         [103., 121., 122., 123., 124., 125., 126., 127.]],\n",
      "\n",
      "        [[104., 129., 130., 131., 132., 133., 134., 135.],\n",
      "         [104., 137., 138., 139., 140., 141., 142., 143.],\n",
      "         [104., 145., 146., 147., 148., 149., 150., 151.],\n",
      "         [104., 153., 154., 155., 156., 157., 158., 159.]],\n",
      "\n",
      "        [[105., 161., 162., 163., 164., 165., 166., 167.],\n",
      "         [105., 169., 170., 171., 172., 173., 174., 175.],\n",
      "         [105., 177., 178., 179., 180., 181., 182., 183.],\n",
      "         [105., 185., 186., 187., 188., 189., 190., 191.]],\n",
      "\n",
      "        [[106., 193., 194., 195., 196., 197., 198., 199.],\n",
      "         [106., 201., 202., 203., 204., 205., 206., 207.],\n",
      "         [106., 209., 210., 211., 212., 213., 214., 215.],\n",
      "         [106., 217., 218., 219., 220., 221., 222., 223.]],\n",
      "\n",
      "        [[107., 225., 226., 227., 228., 229., 230., 231.],\n",
      "         [107., 233., 234., 235., 236., 237., 238., 239.],\n",
      "         [107., 241., 242., 243., 244., 245., 246., 247.],\n",
      "         [107., 249., 250., 251., 252., 253., 254., 255.]],\n",
      "\n",
      "        [[108., 257., 258., 259., 260., 261., 262., 263.],\n",
      "         [108., 265., 266., 267., 268., 269., 270., 271.],\n",
      "         [108., 273., 274., 275., 276., 277., 278., 279.],\n",
      "         [108., 281., 282., 283., 284., 285., 286., 287.]]])\n"
     ]
    }
   ],
   "source": [
    "print(query.shape)\n",
    "print(query)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3008f5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query1 shape: torch.Size([4, 9, 8])\n",
      "query1: tensor([[[100.,   1.,   2.,   3.,   4.,   5.,   6.,   7.],\n",
      "         [101.,  33.,  34.,  35.,  36.,  37.,  38.,  39.],\n",
      "         [102.,  65.,  66.,  67.,  68.,  69.,  70.,  71.],\n",
      "         [103.,  97.,  98.,  99., 100., 101., 102., 103.],\n",
      "         [104., 129., 130., 131., 132., 133., 134., 135.],\n",
      "         [105., 161., 162., 163., 164., 165., 166., 167.],\n",
      "         [106., 193., 194., 195., 196., 197., 198., 199.],\n",
      "         [107., 225., 226., 227., 228., 229., 230., 231.],\n",
      "         [108., 257., 258., 259., 260., 261., 262., 263.]],\n",
      "\n",
      "        [[100.,   9.,  10.,  11.,  12.,  13.,  14.,  15.],\n",
      "         [101.,  41.,  42.,  43.,  44.,  45.,  46.,  47.],\n",
      "         [102.,  73.,  74.,  75.,  76.,  77.,  78.,  79.],\n",
      "         [103., 105., 106., 107., 108., 109., 110., 111.],\n",
      "         [104., 137., 138., 139., 140., 141., 142., 143.],\n",
      "         [105., 169., 170., 171., 172., 173., 174., 175.],\n",
      "         [106., 201., 202., 203., 204., 205., 206., 207.],\n",
      "         [107., 233., 234., 235., 236., 237., 238., 239.],\n",
      "         [108., 265., 266., 267., 268., 269., 270., 271.]],\n",
      "\n",
      "        [[100.,  17.,  18.,  19.,  20.,  21.,  22.,  23.],\n",
      "         [101.,  49.,  50.,  51.,  52.,  53.,  54.,  55.],\n",
      "         [102.,  81.,  82.,  83.,  84.,  85.,  86.,  87.],\n",
      "         [103., 113., 114., 115., 116., 117., 118., 119.],\n",
      "         [104., 145., 146., 147., 148., 149., 150., 151.],\n",
      "         [105., 177., 178., 179., 180., 181., 182., 183.],\n",
      "         [106., 209., 210., 211., 212., 213., 214., 215.],\n",
      "         [107., 241., 242., 243., 244., 245., 246., 247.],\n",
      "         [108., 273., 274., 275., 276., 277., 278., 279.]],\n",
      "\n",
      "        [[100.,  25.,  26.,  27.,  28.,  29.,  30.,  31.],\n",
      "         [101.,  57.,  58.,  59.,  60.,  61.,  62.,  63.],\n",
      "         [102.,  89.,  90.,  91.,  92.,  93.,  94.,  95.],\n",
      "         [103., 121., 122., 123., 124., 125., 126., 127.],\n",
      "         [104., 153., 154., 155., 156., 157., 158., 159.],\n",
      "         [105., 185., 186., 187., 188., 189., 190., 191.],\n",
      "         [106., 217., 218., 219., 220., 221., 222., 223.],\n",
      "         [107., 249., 250., 251., 252., 253., 254., 255.],\n",
      "         [108., 281., 282., 283., 284., 285., 286., 287.]]])\n"
     ]
    }
   ],
   "source": [
    "query1 = query.movedim(0, query.dim() - 2) # (num_tokens, num_heads, head_size) => (num_heads, num_tokens, head_size)\n",
    "print(f\"query1 shape: {query1.shape}\")\n",
    "print(f\"query1: {query1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0bfee3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_q = 0\n",
    "start_kv = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c6aa8f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extend_seq_len_q: 3\n",
      "prefill_seq_len_q: 7\n",
      "seq_len_kv: 10\n",
      "Query range: [0:3]\n",
      "kv range: [0:10]\n",
      "per_req_query shape: torch.Size([4, 3, 8])\n",
      "per_req_query: tensor([[[100.,   1.,   2.,   3.,   4.,   5.,   6.,   7.],\n",
      "         [101.,  33.,  34.,  35.,  36.,  37.,  38.,  39.],\n",
      "         [102.,  65.,  66.,  67.,  68.,  69.,  70.,  71.]],\n",
      "\n",
      "        [[100.,   9.,  10.,  11.,  12.,  13.,  14.,  15.],\n",
      "         [101.,  41.,  42.,  43.,  44.,  45.,  46.,  47.],\n",
      "         [102.,  73.,  74.,  75.,  76.,  77.,  78.,  79.]],\n",
      "\n",
      "        [[100.,  17.,  18.,  19.,  20.,  21.,  22.,  23.],\n",
      "         [101.,  49.,  50.,  51.,  52.,  53.,  54.,  55.],\n",
      "         [102.,  81.,  82.,  83.,  84.,  85.,  86.,  87.]],\n",
      "\n",
      "        [[100.,  25.,  26.,  27.,  28.,  29.,  30.,  31.],\n",
      "         [101.,  57.,  58.,  59.,  60.,  61.,  62.,  63.],\n",
      "         [102.,  89.,  90.,  91.,  92.,  93.,  94.,  95.]]])\n"
     ]
    }
   ],
   "source": [
    "seq_idx = 0\n",
    "\n",
    "extend_seq_len_q = extend_seq_lens[seq_idx]\n",
    "print(f\"extend_seq_len_q: {extend_seq_len_q}\")\n",
    "prefill_seq_len_q = extend_prefix_lens[seq_idx]\n",
    "print(f\"prefill_seq_len_q: {prefill_seq_len_q}\")\n",
    "\n",
    "seq_len_kv = seq_lens[seq_idx]\n",
    "print(f\"seq_len_kv: {seq_len_kv}\")\n",
    "end_q = start_q + extend_seq_len_q\n",
    "end_kv = start_kv + seq_len_kv\n",
    "\n",
    "print(f\"Query range: [{start_q}:{end_q}]\")\n",
    "print(f\"kv range: [{start_kv}:{end_kv}]\")\n",
    "\n",
    "per_req_query = query1[:, start_q:end_q, :]\n",
    "print(f\"per_req_query shape: {per_req_query.shape}\")\n",
    "print(f\"per_req_query: {per_req_query}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "32540150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per_req_query_redudant shape: torch.Size([4, 10, 8])\n",
      "per_req_query_redudant: tensor([[[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "per_req_query_redudant = torch.zeros(\n",
    "                (per_req_query.shape[0], seq_len_kv, per_req_query.shape[2]),\n",
    "                dtype=per_req_query.dtype,\n",
    "                device=per_req_query.device,\n",
    "            )\n",
    "\n",
    "print(f\"per_req_query_redudant shape: {per_req_query_redudant.shape}\")\n",
    "print(f\"per_req_query_redudant: {per_req_query_redudant}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5251a6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per_req_query_redudant shape: torch.Size([4, 10, 8])\n",
      "per_req_query_redudant: tensor([[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [100.,   1.,   2.,   3.,   4.,   5.,   6.,   7.],\n",
      "         [101.,  33.,  34.,  35.,  36.,  37.,  38.,  39.],\n",
      "         [102.,  65.,  66.,  67.,  68.,  69.,  70.,  71.]],\n",
      "\n",
      "        [[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [100.,   9.,  10.,  11.,  12.,  13.,  14.,  15.],\n",
      "         [101.,  41.,  42.,  43.,  44.,  45.,  46.,  47.],\n",
      "         [102.,  73.,  74.,  75.,  76.,  77.,  78.,  79.]],\n",
      "\n",
      "        [[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [100.,  17.,  18.,  19.,  20.,  21.,  22.,  23.],\n",
      "         [101.,  49.,  50.,  51.,  52.,  53.,  54.,  55.],\n",
      "         [102.,  81.,  82.,  83.,  84.,  85.,  86.,  87.]],\n",
      "\n",
      "        [[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [100.,  25.,  26.,  27.,  28.,  29.,  30.,  31.],\n",
      "         [101.,  57.,  58.,  59.,  60.,  61.,  62.,  63.],\n",
      "         [102.,  89.,  90.,  91.,  92.,  93.,  94.,  95.]]])\n"
     ]
    }
   ],
   "source": [
    "per_req_query_redudant[:, prefill_seq_len_q:, :] = per_req_query\n",
    "print(f\"per_req_query_redudant shape: {per_req_query_redudant.shape}\")\n",
    "print(f\"per_req_query_redudant: {per_req_query_redudant}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "838be709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "req_pool_idx: 42\n",
      "per_req_tokens: tensor([100, 101, 102, 103, 104, 105, 106, 107, 108, 109])\n"
     ]
    }
   ],
   "source": [
    "req_pool_idx = req_pool_indices[seq_idx]\n",
    "print(f\"req_pool_idx: {req_pool_idx}\")\n",
    "per_req_tokens = req_to_token[req_pool_idx, :seq_len_kv]\n",
    "print(f\"per_req_tokens: {per_req_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8376c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_req_key = k_cache[per_req_tokens]\n",
    "print(f\"per_req_key shape: {per_req_key.shape}\")\n",
    "print(f\"per_req_key: {per_req_key}\")\n",
    "\n",
    "per_req_key = per_req_key.movedim(0, query.dim() - 2)\n",
    "print(f\"per_req_key shape after move: {per_req_key.shape}\")\n",
    "print(f\"per_req_key: {per_req_key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b2d12d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1100., 3201., 3202., 3203., 3204., 3205., 3206., 3207.],\n",
       "        [1100., 3209., 3210., 3211., 3212., 3213., 3214., 3215.],\n",
       "        [1100., 3217., 3218., 3219., 3220., 3221., 3222., 3223.],\n",
       "        [1100., 3225., 3226., 3227., 3228., 3229., 3230., 3231.]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_cache[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "05efb4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per_req_value shape: torch.Size([10, 4, 8])\n",
      "per_req_value: tensor([[[2100., 3201., 3202., 3203., 3204., 3205., 3206., 3207.],\n",
      "         [2100., 3209., 3210., 3211., 3212., 3213., 3214., 3215.],\n",
      "         [2100., 3217., 3218., 3219., 3220., 3221., 3222., 3223.],\n",
      "         [2100., 3225., 3226., 3227., 3228., 3229., 3230., 3231.]],\n",
      "\n",
      "        [[2101., 3233., 3234., 3235., 3236., 3237., 3238., 3239.],\n",
      "         [2101., 3241., 3242., 3243., 3244., 3245., 3246., 3247.],\n",
      "         [2101., 3249., 3250., 3251., 3252., 3253., 3254., 3255.],\n",
      "         [2101., 3257., 3258., 3259., 3260., 3261., 3262., 3263.]],\n",
      "\n",
      "        [[2102., 3265., 3266., 3267., 3268., 3269., 3270., 3271.],\n",
      "         [2102., 3273., 3274., 3275., 3276., 3277., 3278., 3279.],\n",
      "         [2102., 3281., 3282., 3283., 3284., 3285., 3286., 3287.],\n",
      "         [2102., 3289., 3290., 3291., 3292., 3293., 3294., 3295.]],\n",
      "\n",
      "        [[2103., 3297., 3298., 3299., 3300., 3301., 3302., 3303.],\n",
      "         [2103., 3305., 3306., 3307., 3308., 3309., 3310., 3311.],\n",
      "         [2103., 3313., 3314., 3315., 3316., 3317., 3318., 3319.],\n",
      "         [2103., 3321., 3322., 3323., 3324., 3325., 3326., 3327.]],\n",
      "\n",
      "        [[2104., 3329., 3330., 3331., 3332., 3333., 3334., 3335.],\n",
      "         [2104., 3337., 3338., 3339., 3340., 3341., 3342., 3343.],\n",
      "         [2104., 3345., 3346., 3347., 3348., 3349., 3350., 3351.],\n",
      "         [2104., 3353., 3354., 3355., 3356., 3357., 3358., 3359.]],\n",
      "\n",
      "        [[2105., 3361., 3362., 3363., 3364., 3365., 3366., 3367.],\n",
      "         [2105., 3369., 3370., 3371., 3372., 3373., 3374., 3375.],\n",
      "         [2105., 3377., 3378., 3379., 3380., 3381., 3382., 3383.],\n",
      "         [2105., 3385., 3386., 3387., 3388., 3389., 3390., 3391.]],\n",
      "\n",
      "        [[2106., 3393., 3394., 3395., 3396., 3397., 3398., 3399.],\n",
      "         [2106., 3401., 3402., 3403., 3404., 3405., 3406., 3407.],\n",
      "         [2106., 3409., 3410., 3411., 3412., 3413., 3414., 3415.],\n",
      "         [2106., 3417., 3418., 3419., 3420., 3421., 3422., 3423.]],\n",
      "\n",
      "        [[2107., 3425., 3426., 3427., 3428., 3429., 3430., 3431.],\n",
      "         [2107., 3433., 3434., 3435., 3436., 3437., 3438., 3439.],\n",
      "         [2107., 3441., 3442., 3443., 3444., 3445., 3446., 3447.],\n",
      "         [2107., 3449., 3450., 3451., 3452., 3453., 3454., 3455.]],\n",
      "\n",
      "        [[2108., 3457., 3458., 3459., 3460., 3461., 3462., 3463.],\n",
      "         [2108., 3465., 3466., 3467., 3468., 3469., 3470., 3471.],\n",
      "         [2108., 3473., 3474., 3475., 3476., 3477., 3478., 3479.],\n",
      "         [2108., 3481., 3482., 3483., 3484., 3485., 3486., 3487.]],\n",
      "\n",
      "        [[2109., 3489., 3490., 3491., 3492., 3493., 3494., 3495.],\n",
      "         [2109., 3497., 3498., 3499., 3500., 3501., 3502., 3503.],\n",
      "         [2109., 3505., 3506., 3507., 3508., 3509., 3510., 3511.],\n",
      "         [2109., 3513., 3514., 3515., 3516., 3517., 3518., 3519.]]])\n",
      "per_req_value shape after move: torch.Size([4, 10, 8])\n",
      "per_req_value: tensor([[[2100., 3201., 3202., 3203., 3204., 3205., 3206., 3207.],\n",
      "         [2101., 3233., 3234., 3235., 3236., 3237., 3238., 3239.],\n",
      "         [2102., 3265., 3266., 3267., 3268., 3269., 3270., 3271.],\n",
      "         [2103., 3297., 3298., 3299., 3300., 3301., 3302., 3303.],\n",
      "         [2104., 3329., 3330., 3331., 3332., 3333., 3334., 3335.],\n",
      "         [2105., 3361., 3362., 3363., 3364., 3365., 3366., 3367.],\n",
      "         [2106., 3393., 3394., 3395., 3396., 3397., 3398., 3399.],\n",
      "         [2107., 3425., 3426., 3427., 3428., 3429., 3430., 3431.],\n",
      "         [2108., 3457., 3458., 3459., 3460., 3461., 3462., 3463.],\n",
      "         [2109., 3489., 3490., 3491., 3492., 3493., 3494., 3495.]],\n",
      "\n",
      "        [[2100., 3209., 3210., 3211., 3212., 3213., 3214., 3215.],\n",
      "         [2101., 3241., 3242., 3243., 3244., 3245., 3246., 3247.],\n",
      "         [2102., 3273., 3274., 3275., 3276., 3277., 3278., 3279.],\n",
      "         [2103., 3305., 3306., 3307., 3308., 3309., 3310., 3311.],\n",
      "         [2104., 3337., 3338., 3339., 3340., 3341., 3342., 3343.],\n",
      "         [2105., 3369., 3370., 3371., 3372., 3373., 3374., 3375.],\n",
      "         [2106., 3401., 3402., 3403., 3404., 3405., 3406., 3407.],\n",
      "         [2107., 3433., 3434., 3435., 3436., 3437., 3438., 3439.],\n",
      "         [2108., 3465., 3466., 3467., 3468., 3469., 3470., 3471.],\n",
      "         [2109., 3497., 3498., 3499., 3500., 3501., 3502., 3503.]],\n",
      "\n",
      "        [[2100., 3217., 3218., 3219., 3220., 3221., 3222., 3223.],\n",
      "         [2101., 3249., 3250., 3251., 3252., 3253., 3254., 3255.],\n",
      "         [2102., 3281., 3282., 3283., 3284., 3285., 3286., 3287.],\n",
      "         [2103., 3313., 3314., 3315., 3316., 3317., 3318., 3319.],\n",
      "         [2104., 3345., 3346., 3347., 3348., 3349., 3350., 3351.],\n",
      "         [2105., 3377., 3378., 3379., 3380., 3381., 3382., 3383.],\n",
      "         [2106., 3409., 3410., 3411., 3412., 3413., 3414., 3415.],\n",
      "         [2107., 3441., 3442., 3443., 3444., 3445., 3446., 3447.],\n",
      "         [2108., 3473., 3474., 3475., 3476., 3477., 3478., 3479.],\n",
      "         [2109., 3505., 3506., 3507., 3508., 3509., 3510., 3511.]],\n",
      "\n",
      "        [[2100., 3225., 3226., 3227., 3228., 3229., 3230., 3231.],\n",
      "         [2101., 3257., 3258., 3259., 3260., 3261., 3262., 3263.],\n",
      "         [2102., 3289., 3290., 3291., 3292., 3293., 3294., 3295.],\n",
      "         [2103., 3321., 3322., 3323., 3324., 3325., 3326., 3327.],\n",
      "         [2104., 3353., 3354., 3355., 3356., 3357., 3358., 3359.],\n",
      "         [2105., 3385., 3386., 3387., 3388., 3389., 3390., 3391.],\n",
      "         [2106., 3417., 3418., 3419., 3420., 3421., 3422., 3423.],\n",
      "         [2107., 3449., 3450., 3451., 3452., 3453., 3454., 3455.],\n",
      "         [2108., 3481., 3482., 3483., 3484., 3485., 3486., 3487.],\n",
      "         [2109., 3513., 3514., 3515., 3516., 3517., 3518., 3519.]]])\n"
     ]
    }
   ],
   "source": [
    "per_req_value = v_cache[per_req_tokens]\n",
    "print(f\"per_req_value shape: {per_req_value.shape}\")\n",
    "print(f\"per_req_value: {per_req_value}\")\n",
    "per_req_value = per_req_value.movedim(0, query.dim() - 2)\n",
    "print(f\"per_req_value shape after move: {per_req_value.shape}\")\n",
    "print(f\"per_req_value: {per_req_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "55b512d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atten shape: torch.Size([1, 4, 10, 8])\n",
      "atten shape after move: torch.Size([10, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "atten = scaled_dot_product_attention(\n",
    "                    per_req_query_redudant.unsqueeze(0),\n",
    "                    per_req_key.unsqueeze(0),\n",
    "                    per_req_value.unsqueeze(0),\n",
    "                    enable_gqa=enable_gqa,\n",
    "                    scale=scaling,\n",
    "                    is_causal=causal)\n",
    "\n",
    "print(f\"atten shape: {atten.shape}\")\n",
    "# print(f\"atten: {atten}\")\n",
    "\n",
    "atten = atten.squeeze(0).movedim(query.dim() - 2, 0)\n",
    "print(f\"atten shape after move: {atten.shape}\")\n",
    "# print(f\"atten: {atten}\")\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5baa3891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefill_seq_len_q: 7\n",
      "x shape: torch.Size([3, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "print(f\"prefill_seq_len_q: {prefill_seq_len_q}\")\n",
    "x = atten[prefill_seq_len_q:, :, :] \n",
    "print(f\"x shape: {x.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
