{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef3e9a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c2cf9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q shape: torch.Size([2, 4, 5, 4])\n",
      "K shape: torch.Size([2, 4, 5, 4])\n",
      "V shape: torch.Size([2, 4, 5, 4])\n",
      "V: tensor([[[[  0.,   1.,   2.,   3.],\n",
      "          [  4.,   5.,   6.,   7.],\n",
      "          [  8.,   9.,  10.,  11.],\n",
      "          [ 12.,  13.,  14.,  15.],\n",
      "          [ 16.,  17.,  18.,  19.]],\n",
      "\n",
      "         [[ 20.,  21.,  22.,  23.],\n",
      "          [ 24.,  25.,  26.,  27.],\n",
      "          [ 28.,  29.,  30.,  31.],\n",
      "          [ 32.,  33.,  34.,  35.],\n",
      "          [ 36.,  37.,  38.,  39.]],\n",
      "\n",
      "         [[ 40.,  41.,  42.,  43.],\n",
      "          [ 44.,  45.,  46.,  47.],\n",
      "          [ 48.,  49.,  50.,  51.],\n",
      "          [ 52.,  53.,  54.,  55.],\n",
      "          [ 56.,  57.,  58.,  59.]],\n",
      "\n",
      "         [[ 60.,  61.,  62.,  63.],\n",
      "          [ 64.,  65.,  66.,  67.],\n",
      "          [ 68.,  69.,  70.,  71.],\n",
      "          [ 72.,  73.,  74.,  75.],\n",
      "          [ 76.,  77.,  78.,  79.]]],\n",
      "\n",
      "\n",
      "        [[[ 80.,  81.,  82.,  83.],\n",
      "          [ 84.,  85.,  86.,  87.],\n",
      "          [ 88.,  89.,  90.,  91.],\n",
      "          [ 92.,  93.,  94.,  95.],\n",
      "          [ 96.,  97.,  98.,  99.]],\n",
      "\n",
      "         [[100., 101., 102., 103.],\n",
      "          [104., 105., 106., 107.],\n",
      "          [108., 109., 110., 111.],\n",
      "          [112., 113., 114., 115.],\n",
      "          [116., 117., 118., 119.]],\n",
      "\n",
      "         [[120., 121., 122., 123.],\n",
      "          [124., 125., 126., 127.],\n",
      "          [128., 129., 130., 131.],\n",
      "          [132., 133., 134., 135.],\n",
      "          [136., 137., 138., 139.]],\n",
      "\n",
      "         [[140., 141., 142., 143.],\n",
      "          [144., 145., 146., 147.],\n",
      "          [148., 149., 150., 151.],\n",
      "          [152., 153., 154., 155.],\n",
      "          [156., 157., 158., 159.]]]])\n",
      "Self-attention output shape: torch.Size([2, 4, 5, 4])\n",
      "Self-attention output: tensor([[[[  0.,   1.,   2.,   3.],\n",
      "          [  4.,   5.,   6.,   7.],\n",
      "          [  8.,   9.,  10.,  11.],\n",
      "          [ 12.,  13.,  14.,  15.],\n",
      "          [ 16.,  17.,  18.,  19.]],\n",
      "\n",
      "         [[ 20.,  21.,  22.,  23.],\n",
      "          [ 24.,  25.,  26.,  27.],\n",
      "          [ 28.,  29.,  30.,  31.],\n",
      "          [ 32.,  33.,  34.,  35.],\n",
      "          [ 36.,  37.,  38.,  39.]],\n",
      "\n",
      "         [[ 40.,  41.,  42.,  43.],\n",
      "          [ 44.,  45.,  46.,  47.],\n",
      "          [ 48.,  49.,  50.,  51.],\n",
      "          [ 52.,  53.,  54.,  55.],\n",
      "          [ 56.,  57.,  58.,  59.]],\n",
      "\n",
      "         [[ 60.,  61.,  62.,  63.],\n",
      "          [ 64.,  65.,  66.,  67.],\n",
      "          [ 68.,  69.,  70.,  71.],\n",
      "          [ 72.,  73.,  74.,  75.],\n",
      "          [ 76.,  77.,  78.,  79.]]],\n",
      "\n",
      "\n",
      "        [[[ 80.,  81.,  82.,  83.],\n",
      "          [ 84.,  85.,  86.,  87.],\n",
      "          [ 88.,  89.,  90.,  91.],\n",
      "          [ 92.,  93.,  94.,  95.],\n",
      "          [ 96.,  97.,  98.,  99.]],\n",
      "\n",
      "         [[100., 101., 102., 103.],\n",
      "          [104., 105., 106., 107.],\n",
      "          [108., 109., 110., 111.],\n",
      "          [112., 113., 114., 115.],\n",
      "          [116., 117., 118., 119.]],\n",
      "\n",
      "         [[120., 121., 122., 123.],\n",
      "          [124., 125., 126., 127.],\n",
      "          [128., 129., 130., 131.],\n",
      "          [132., 133., 134., 135.],\n",
      "          [136., 137., 138., 139.]],\n",
      "\n",
      "         [[140., 141., 142., 143.],\n",
      "          [144., 145., 146., 147.],\n",
      "          [148., 149., 150., 151.],\n",
      "          [152., 153., 154., 155.],\n",
      "          [156., 157., 158., 159.]]]])\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "batch_size = 2\n",
    "seq_len = 5       # same for Q, K, V in self-attention\n",
    "embed_dim = 16    # total embedding size\n",
    "num_heads = 4\n",
    "head_dim = embed_dim // num_heads\n",
    "\n",
    "\n",
    "Q = torch.arange(batch_size * num_heads * seq_len * head_dim).view(batch_size, num_heads, seq_len, head_dim).float()\n",
    "K = torch.arange(batch_size * num_heads * seq_len * head_dim).view(batch_size, num_heads, seq_len, head_dim).float()\n",
    "V = torch.arange(batch_size * num_heads * seq_len * head_dim).view(batch_size, num_heads, seq_len, head_dim).float()\n",
    "\n",
    "print(\"Q shape:\", Q.shape)\n",
    "print(\"K shape:\", K.shape)  \n",
    "print(\"V shape:\", V.shape)\n",
    "\n",
    "#print(\"Q:\", Q)\n",
    "#print(\"K:\", K)\n",
    "print(\"V:\", V)\n",
    "\n",
    "# Optional causal mask (prevent attending to future positions)\n",
    "causal_mask = torch.ones(seq_len, seq_len, dtype=torch.bool).tril()\n",
    "\n",
    "# Self-attention\n",
    "out = F.scaled_dot_product_attention(\n",
    "    Q, K, V,\n",
    "    enable_gqa=False,  \n",
    "    scale=None,        \n",
    "    is_causal=True,  \n",
    ")\n",
    "\n",
    "print(\"Self-attention output shape:\", out.shape)\n",
    "print(\"Self-attention output:\", out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef50f8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_ shape: torch.Size([5, 4])\n",
      "Q_: tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]])\n",
      "Raw scores shape: torch.Size([5, 5])\n",
      "Raw scores: tensor([[  14.,   38.,   62.,   86.,  110.],\n",
      "        [  38.,  126.,  214.,  302.,  390.],\n",
      "        [  62.,  214.,  366.,  518.,  670.],\n",
      "        [  86.,  302.,  518.,  734.,  950.],\n",
      "        [ 110.,  390.,  670.,  950., 1230.]])\n",
      "Scaled scores shape: torch.Size([5, 5])\n",
      "Scaled scores: tensor([[  7.,  19.,  31.,  43.,  55.],\n",
      "        [ 19.,  63., 107., 151., 195.],\n",
      "        [ 31., 107., 183., 259., 335.],\n",
      "        [ 43., 151., 259., 367., 475.],\n",
      "        [ 55., 195., 335., 475., 615.]])\n",
      "Mask shape: torch.Size([5, 5])\n",
      "Mask: tensor([[ True, False, False, False, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True]])\n",
      "Masked scores shape: torch.Size([5, 5])\n",
      "Masked scores: tensor([[  7., -inf, -inf, -inf, -inf],\n",
      "        [ 19.,  63., -inf, -inf, -inf],\n",
      "        [ 31., 107., 183., -inf, -inf],\n",
      "        [ 43., 151., 259., 367., -inf],\n",
      "        [ 55., 195., 335., 475., 615.]])\n",
      "Attention weights shape: torch.Size([5, 5])\n",
      "Attention weights: tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [7.7811e-20, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 9.8542e-34, 1.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00]])\n",
      "Sum of attention weights shape: torch.Size([5, 1])\n",
      "Sum of attention weights: tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "out_manual: tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]])\n"
     ]
    }
   ],
   "source": [
    "b, h = 0, 0\n",
    "Q_ = Q[b, h]  # [5,4]\n",
    "K_ = K[b, h]  # [5,4]\n",
    "V_ = V[b, h]  # [5,4]\n",
    "\n",
    "print(\"Q_ shape:\", Q_.shape)\n",
    "print(\"Q_:\", Q_)\n",
    "\n",
    "# Step 1: raw scores\n",
    "scores = Q_ @ K_.T\n",
    "\n",
    "print(\"Raw scores shape:\", scores.shape)\n",
    "print(\"Raw scores:\", scores)\n",
    "\n",
    "# Step 2: scale\n",
    "\n",
    "scores_scaled = scores / math.sqrt(Q_.shape[1])\n",
    "\n",
    "print(\"Scaled scores shape:\", scores_scaled.shape)\n",
    "print(\"Scaled scores:\", scores_scaled)\n",
    "\n",
    "# Step 3: causal mask\n",
    "mask = torch.tril(torch.ones(Q_.shape[0], Q_.shape[0], dtype=torch.bool))\n",
    "print(\"Mask shape:\", mask.shape)\n",
    "print(\"Mask:\", mask)\n",
    "\n",
    "scores_scaled = scores_scaled.masked_fill(~mask, float('-inf'))\n",
    "print(\"Masked scores shape:\", scores_scaled.shape)\n",
    "print(\"Masked scores:\", scores_scaled)\n",
    "\n",
    "# Step 4: softmax\n",
    "attn_weights = F.softmax(scores_scaled, dim=-1)\n",
    "print(\"Attention weights shape:\", attn_weights.shape)\n",
    "print(\"Attention weights:\", attn_weights)\n",
    "\n",
    "sum = attn_weights.sum(dim=-1, keepdim=True)  # Normalize weights\n",
    "print(\"Sum of attention weights shape:\", sum.shape)\n",
    "print(\"Sum of attention weights:\", sum)\n",
    "\n",
    "# Step 5: weighted sum\n",
    "out_manual = attn_weights @ V_\n",
    "print(\"out_manual:\", out_manual)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
